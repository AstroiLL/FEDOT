{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "from core.composer.gp_composer.gp_composer import \\\n",
    "    GPComposer, GPComposerRequirements\n",
    "from core.composer.visualisation import ComposerVisualiser\n",
    "from core.repository.model_types_repository import ModelTypesRepository\n",
    "from core.repository.quality_metrics_repository import \\\n",
    "    ClassificationMetricsEnum, MetricsRepository\n",
    "from core.repository.tasks import Task, TaskTypesEnum\n",
    "from core.utils import probs_to_labels\n",
    "from examples.utils import create_multi_clf_examples_from_excel\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score as roc_auc\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report, confusion_matrix\n",
    "from benchmark.benchmark_utils import get_scoring_case_data_paths\n",
    "from core.composer.chain import Chain\n",
    "from core.composer.node import PrimaryNode, SecondaryNode\n",
    "from core.models.data import InputData\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "def get_model(train_file_path: str, cur_lead_time: datetime.timedelta = timedelta(minutes=5)):\n",
    "    task = Task(task_type=TaskTypesEnum.classification)\n",
    "    dataset_to_compose = InputData.from_csv(train_file_path, task=task)\n",
    "\n",
    "    # the search of the models provided by the framework\n",
    "    # that can be used as nodes in a chain for the selected task\n",
    "    models_repo = ModelTypesRepository()\n",
    "    available_model_types, _ = models_repo.suitable_model(task_type=task.task_type)\n",
    "\n",
    "    metric_function = MetricsRepository(). \\\n",
    "        metric_by_id(ClassificationMetricsEnum.ROCAUC_penalty)\n",
    "\n",
    "    composer_requirements = GPComposerRequirements(\n",
    "        primary=available_model_types, secondary=available_model_types,\n",
    "        max_lead_time=cur_lead_time, max_arity=3,\n",
    "        max_depth=4, pop_size=20, num_of_generations=100, \n",
    "        crossover_prob = 0.8, mutation_prob = 0.8, \n",
    "        add_single_model_chains = True)\n",
    "\n",
    "    # Create the genetic programming-based composer, that allow to find\n",
    "    # the optimal structure of the composite model\n",
    "    composer = GPComposer()\n",
    "\n",
    "    # run the search of best suitable model\n",
    "    chain_evo_composed = composer.compose_chain(data=dataset_to_compose,\n",
    "                                                initial_chain=None,\n",
    "                                                composer_requirements=composer_requirements,\n",
    "                                                metrics=metric_function, is_visualise=False)\n",
    "    \n",
    "    chain_evo_composed.fit(input_data=dataset_to_compose)\n",
    "\n",
    "    return chain_evo_composed\n",
    "\n",
    "\n",
    "def apply_model_to_data(model: Chain, data_path: str):\n",
    "    df, file_path = create_multi_clf_examples_from_excel(data_path, return_df=True)\n",
    "    dataset_to_apply = InputData.from_csv(file_path, with_target=True)\n",
    "    evo_predicted = model.predict(dataset_to_apply)\n",
    "    df['forecast'] = probs_to_labels(evo_predicted.predict)\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_model_to_data_and_predict(model: Chain, data_path: str):\n",
    "    df, file_path = create_multi_clf_examples_from_excel(data_path, return_df=True)\n",
    "    \n",
    "    dataset_to_validate = InputData.from_csv(data_path)\n",
    "    predicted_labels = model.predict(dataset_to_validate).predict\n",
    "    \n",
    "    \n",
    "    test_data = InputData.from_csv(file_path, with_target=True)\n",
    "    roc_auc_valid = roc_auc(y_true=test_data.target,\n",
    "                                  y_score=predicted_labels,\n",
    "                                  multi_class='ovo',\n",
    "                                  average='macro')\n",
    "    \n",
    "    roc_auc_st = roc_auc(y_true=test_data.target, y_score=predicted_labels.round())\n",
    "    \n",
    "    p = precision_score(y_true=test_data.target,y_pred=predicted_labels.round())\n",
    "    r = recall_score(y_true=test_data.target, y_pred=predicted_labels.round())\n",
    "    a = accuracy_score(y_true=test_data.target, y_pred=predicted_labels.round())\n",
    "    \n",
    "    return roc_auc_valid, roc_auc_st, p, r, a\n",
    "\n",
    "\n",
    "def validate_model_quality(model: Chain, data_path: str):\n",
    "    dataset_to_validate = InputData.from_csv(data_path)\n",
    "    predicted_labels = model.predict(dataset_to_validate).predict\n",
    "\n",
    "    roc_auc_valid = round(roc_auc(y_true=test_data.target,\n",
    "                                  y_score=predicted_labels,\n",
    "                                  multi_class='ovo',\n",
    "                                  average='macro'), 3)\n",
    "    \n",
    "    roc_auc_st = roc_auc(y_true=test_data.target,y_score=predicted_labels)\n",
    "                              \n",
    "    p = precision_score(y_true=test_data.target,y_pred=predicted_labels.round())\n",
    "    r = recall_score(y_true=test_data.target, y_pred=predicted_labels.round())\n",
    "    a = accuracy_score(y_true=test_data.target, y_pred=predicted_labels.round())\n",
    "    \n",
    "    return roc_auc_valid, roc_auc_st, p, r, a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in chain assessment during composition: Invalid chain configuration: Chain has incorrect models positions. Continue.\n",
      "Error in chain assessment during composition: Invalid chain configuration: Chain has incorrect models positions. Continue.\n",
      "Generation num: 0\n",
      "spent time: 1.6 min\n",
      "Best metric is -0.9976666666666667\n",
      "Generation num: 1\n",
      "spent time: 2.5 min\n",
      "Best metric is -0.9976666666666667\n",
      "Generation num: 2\n",
      "spent time: 3.1 min\n",
      "Best metric is -0.9976666666666667\n",
      "Generation num: 3\n",
      "spent time: 3.6 min\n",
      "Best metric is -0.9976666666666667\n",
      "Generation num: 4\n",
      "spent time: 4.2 min\n",
      "Best metric is -0.9976666666666667\n",
      "Generation num: 5\n",
      "spent time: 4.7 min\n",
      "Best metric is -0.9976666666666667\n",
      "Composition time: 4.683 min\n",
      "Algorithm was terminated due to processing time limit\n",
      "GP composition finished\n",
      "ROC AUC metric is 0.97, \n",
      "ROC_AUC_ALL 0.9695924764890282, \n",
      "PRECISION is 0.9759036144578314, \n",
      "RECALL is 0.9310344827586207, \n",
      "ACCURACY is 0.9593908629441624\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1152 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path_first = r'./creditcard_scaling_underSample.csv'\n",
    "# file_path_first = r'./creditcard_under.csv'\n",
    "# file_path_first = r'./creditcard.csv'\n",
    "\n",
    "#examples/utils.py replace read_excel => read_csv\n",
    "train_file_path, test_file_path = create_multi_clf_examples_from_excel(file_path_first)\n",
    "test_data = InputData.from_csv(test_file_path)\n",
    "#Error!!!!\n",
    "fitted_model = get_model(train_file_path)\n",
    "\n",
    "ComposerVisualiser.visualise(fitted_model, save_path = f'./model6.jpg')\n",
    "\n",
    "roc_auc, roc, p, r, a = validate_model_quality(fitted_model, test_file_path)\n",
    "print(f'ROC AUC metric is {roc_auc}, \\nROC_AUC_ALL {roc}, \\nPRECISION is {p}, \\nRECALL is {r}, \\nACCURACY is {a}')\n",
    "\n",
    "#0.972 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC metric is 0.9755855858947243, ROC_AUC_ALL 0.9618787253459999, PRECISION is 0.06067298947641939, RECALL is 0.9491869918699187, ACCURACY is 0.974526609247666\n"
     ]
    }
   ],
   "source": [
    "file_path_second = r'./creditcard_scaling.csv'\n",
    "# file_path_second = r'./examples/data/creditcard_under/test.csv'\n",
    "roc, roc_auc_all, p, r, a = apply_model_to_data_and_predict(fitted_model, file_path_second)\n",
    "print(f'ROC AUC metric is {roc}, ROC_AUC_ALL {roc_auc_all}, PRECISION is {p}, RECALL is {r}, ACCURACY is {a}')\n",
    "#ROC AUC metric is 0.969, ROC_AUC_ALL 0.9693877551020409, PRECISION is 0.9263157894736842, RECALL is 0.8979591836734694, ACCURACY is 0.9137055837563451\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.293440</td>\n",
       "      <td>-0.906366</td>\n",
       "      <td>0.329594</td>\n",
       "      <td>3.712889</td>\n",
       "      <td>-5.775935</td>\n",
       "      <td>6.078266</td>\n",
       "      <td>1.667359</td>\n",
       "      <td>-2.420168</td>\n",
       "      <td>-0.812891</td>\n",
       "      <td>0.133080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269773</td>\n",
       "      <td>0.156617</td>\n",
       "      <td>-0.652450</td>\n",
       "      <td>-0.551572</td>\n",
       "      <td>-0.716522</td>\n",
       "      <td>1.415717</td>\n",
       "      <td>0.555265</td>\n",
       "      <td>0.530507</td>\n",
       "      <td>0.404474</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.473276</td>\n",
       "      <td>0.553225</td>\n",
       "      <td>-1.099270</td>\n",
       "      <td>-0.401308</td>\n",
       "      <td>0.979359</td>\n",
       "      <td>-1.885112</td>\n",
       "      <td>-0.250168</td>\n",
       "      <td>-0.133203</td>\n",
       "      <td>0.545923</td>\n",
       "      <td>0.112715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618658</td>\n",
       "      <td>0.400881</td>\n",
       "      <td>0.679017</td>\n",
       "      <td>-0.078751</td>\n",
       "      <td>-0.520938</td>\n",
       "      <td>0.773371</td>\n",
       "      <td>-0.003025</td>\n",
       "      <td>-0.029610</td>\n",
       "      <td>0.089475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.296653</td>\n",
       "      <td>0.894959</td>\n",
       "      <td>-0.644278</td>\n",
       "      <td>5.002352</td>\n",
       "      <td>-8.252739</td>\n",
       "      <td>7.756915</td>\n",
       "      <td>-0.216267</td>\n",
       "      <td>-2.751496</td>\n",
       "      <td>-3.358857</td>\n",
       "      <td>1.406268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816558</td>\n",
       "      <td>0.587728</td>\n",
       "      <td>-0.605759</td>\n",
       "      <td>0.033746</td>\n",
       "      <td>-0.756170</td>\n",
       "      <td>-0.008172</td>\n",
       "      <td>0.532772</td>\n",
       "      <td>0.663970</td>\n",
       "      <td>0.192067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.181793</td>\n",
       "      <td>-0.294541</td>\n",
       "      <td>1.245674</td>\n",
       "      <td>0.166975</td>\n",
       "      <td>0.488306</td>\n",
       "      <td>0.635322</td>\n",
       "      <td>-0.562777</td>\n",
       "      <td>-1.011073</td>\n",
       "      <td>0.014953</td>\n",
       "      <td>-0.160211</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.132080</td>\n",
       "      <td>-0.262581</td>\n",
       "      <td>-0.816264</td>\n",
       "      <td>0.140304</td>\n",
       "      <td>0.357827</td>\n",
       "      <td>0.186423</td>\n",
       "      <td>0.096544</td>\n",
       "      <td>-0.035866</td>\n",
       "      <td>0.018495</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.293440</td>\n",
       "      <td>-0.087536</td>\n",
       "      <td>-5.129937</td>\n",
       "      <td>5.175102</td>\n",
       "      <td>-3.134030</td>\n",
       "      <td>1.116648</td>\n",
       "      <td>-1.694747</td>\n",
       "      <td>2.070258</td>\n",
       "      <td>-7.153496</td>\n",
       "      <td>-10.700534</td>\n",
       "      <td>...</td>\n",
       "      <td>2.684476</td>\n",
       "      <td>-5.912426</td>\n",
       "      <td>2.246010</td>\n",
       "      <td>1.010550</td>\n",
       "      <td>-0.567496</td>\n",
       "      <td>-0.610164</td>\n",
       "      <td>-0.410800</td>\n",
       "      <td>-0.915947</td>\n",
       "      <td>0.122155</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>0.525816</td>\n",
       "      <td>0.488128</td>\n",
       "      <td>1.967561</td>\n",
       "      <td>-1.212796</td>\n",
       "      <td>0.005362</td>\n",
       "      <td>-0.465616</td>\n",
       "      <td>-1.415820</td>\n",
       "      <td>0.061468</td>\n",
       "      <td>-1.329188</td>\n",
       "      <td>0.146808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.442374</td>\n",
       "      <td>-0.255835</td>\n",
       "      <td>-0.246727</td>\n",
       "      <td>0.300866</td>\n",
       "      <td>-0.341804</td>\n",
       "      <td>-0.706497</td>\n",
       "      <td>0.481806</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>-0.036923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>0.320687</td>\n",
       "      <td>0.458429</td>\n",
       "      <td>-1.195804</td>\n",
       "      <td>-0.257156</td>\n",
       "      <td>0.237545</td>\n",
       "      <td>-2.897071</td>\n",
       "      <td>1.398154</td>\n",
       "      <td>-1.497629</td>\n",
       "      <td>1.078211</td>\n",
       "      <td>-0.134459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053342</td>\n",
       "      <td>0.080830</td>\n",
       "      <td>0.010164</td>\n",
       "      <td>-0.479856</td>\n",
       "      <td>-0.477132</td>\n",
       "      <td>1.290695</td>\n",
       "      <td>-0.078257</td>\n",
       "      <td>-0.082718</td>\n",
       "      <td>0.021433</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>4.445050</td>\n",
       "      <td>-0.511684</td>\n",
       "      <td>-4.595617</td>\n",
       "      <td>5.083690</td>\n",
       "      <td>-7.581015</td>\n",
       "      <td>7.546033</td>\n",
       "      <td>-6.949165</td>\n",
       "      <td>-1.729185</td>\n",
       "      <td>-8.190192</td>\n",
       "      <td>2.714670</td>\n",
       "      <td>...</td>\n",
       "      <td>1.682160</td>\n",
       "      <td>2.248971</td>\n",
       "      <td>0.566844</td>\n",
       "      <td>0.033744</td>\n",
       "      <td>0.591783</td>\n",
       "      <td>0.334229</td>\n",
       "      <td>0.386801</td>\n",
       "      <td>2.163898</td>\n",
       "      <td>0.983104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>1.174736</td>\n",
       "      <td>-0.961865</td>\n",
       "      <td>-1.199752</td>\n",
       "      <td>-0.820796</td>\n",
       "      <td>0.778836</td>\n",
       "      <td>1.687205</td>\n",
       "      <td>1.887818</td>\n",
       "      <td>1.124627</td>\n",
       "      <td>-0.231829</td>\n",
       "      <td>0.621685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.516898</td>\n",
       "      <td>0.470337</td>\n",
       "      <td>1.167774</td>\n",
       "      <td>0.486281</td>\n",
       "      <td>-0.982305</td>\n",
       "      <td>-0.798267</td>\n",
       "      <td>-0.084967</td>\n",
       "      <td>0.232419</td>\n",
       "      <td>0.213574</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>-0.209600</td>\n",
       "      <td>-0.603684</td>\n",
       "      <td>-0.884523</td>\n",
       "      <td>0.201257</td>\n",
       "      <td>0.463587</td>\n",
       "      <td>-1.944052</td>\n",
       "      <td>-0.084804</td>\n",
       "      <td>-0.665372</td>\n",
       "      <td>-0.262911</td>\n",
       "      <td>0.517698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115880</td>\n",
       "      <td>0.338629</td>\n",
       "      <td>0.666223</td>\n",
       "      <td>-0.256702</td>\n",
       "      <td>-0.500831</td>\n",
       "      <td>0.003893</td>\n",
       "      <td>-0.286288</td>\n",
       "      <td>0.227049</td>\n",
       "      <td>0.068341</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>984 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
       "0        -0.293440    -0.906366  0.329594  3.712889 -5.775935  6.078266   \n",
       "1         2.473276     0.553225 -1.099270 -0.401308  0.979359 -1.885112   \n",
       "2        -0.296653     0.894959 -0.644278  5.002352 -8.252739  7.756915   \n",
       "3        -0.181793    -0.294541  1.245674  0.166975  0.488306  0.635322   \n",
       "4        -0.293440    -0.087536 -5.129937  5.175102 -3.134030  1.116648   \n",
       "..             ...          ...       ...       ...       ...       ...   \n",
       "979       0.525816     0.488128  1.967561 -1.212796  0.005362 -0.465616   \n",
       "980       0.320687     0.458429 -1.195804 -0.257156  0.237545 -2.897071   \n",
       "981       4.445050    -0.511684 -4.595617  5.083690 -7.581015  7.546033   \n",
       "982       1.174736    -0.961865 -1.199752 -0.820796  0.778836  1.687205   \n",
       "983      -0.209600    -0.603684 -0.884523  0.201257  0.463587 -1.944052   \n",
       "\n",
       "           V5        V6        V7         V8  ...       V20       V21  \\\n",
       "0    1.667359 -2.420168 -0.812891   0.133080  ...  0.269773  0.156617   \n",
       "1   -0.250168 -0.133203  0.545923   0.112715  ...  0.618658  0.400881   \n",
       "2   -0.216267 -2.751496 -3.358857   1.406268  ...  0.816558  0.587728   \n",
       "3   -0.562777 -1.011073  0.014953  -0.160211  ... -0.132080 -0.262581   \n",
       "4   -1.694747  2.070258 -7.153496 -10.700534  ...  2.684476 -5.912426   \n",
       "..        ...       ...       ...        ...  ...       ...       ...   \n",
       "979 -1.415820  0.061468 -1.329188   0.146808  ... -0.442374 -0.255835   \n",
       "980  1.398154 -1.497629  1.078211  -0.134459  ...  0.053342  0.080830   \n",
       "981 -6.949165 -1.729185 -8.190192   2.714670  ...  1.682160  2.248971   \n",
       "982  1.887818  1.124627 -0.231829   0.621685  ...  0.516898  0.470337   \n",
       "983 -0.084804 -0.665372 -0.262911   0.517698  ...  0.115880  0.338629   \n",
       "\n",
       "          V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0   -0.652450 -0.551572 -0.716522  1.415717  0.555265  0.530507  0.404474   \n",
       "1    0.679017 -0.078751 -0.520938  0.773371 -0.003025 -0.029610  0.089475   \n",
       "2   -0.605759  0.033746 -0.756170 -0.008172  0.532772  0.663970  0.192067   \n",
       "3   -0.816264  0.140304  0.357827  0.186423  0.096544 -0.035866  0.018495   \n",
       "4    2.246010  1.010550 -0.567496 -0.610164 -0.410800 -0.915947  0.122155   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "979 -0.246727  0.300866 -0.341804 -0.706497  0.481806  0.002202 -0.036923   \n",
       "980  0.010164 -0.479856 -0.477132  1.290695 -0.078257 -0.082718  0.021433   \n",
       "981  0.566844  0.033744  0.591783  0.334229  0.386801  2.163898  0.983104   \n",
       "982  1.167774  0.486281 -0.982305 -0.798267 -0.084967  0.232419  0.213574   \n",
       "983  0.666223 -0.256702 -0.500831  0.003893 -0.286288  0.227049  0.068341   \n",
       "\n",
       "     Class  \n",
       "0        1  \n",
       "1        0  \n",
       "2        1  \n",
       "3        0  \n",
       "4        0  \n",
       "..     ...  \n",
       "979      0  \n",
       "980      0  \n",
       "981      1  \n",
       "982      0  \n",
       "983      0  \n",
       "\n",
       "[984 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dat = pd.read_csv('creditcard_scaling_underSample.csv')\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df = shuffle(dat, random_state = 42).reset_index().drop(columns='index')\n",
    "\n",
    "df.to_csv(r'.\\creditcard_under_shuffle.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = InputData.from_csv(file_path_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC AUC metric is 0.975, PRECISION is 0.946236559139785, RECALL is 0.8979591836734694, ACCURACY is 0.9238578680203046 // model5\n",
    "\n",
    "ROC AUC metric is 0.972, PRECISION is 0.9263157894736842, RECALL is 0.8979591836734694, ACCURACY is 0.9137055837563451 // model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9926783333333333"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_model.fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9926783333333333"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_model.fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9886876666666666"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_model.fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9926783333333333"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_model.fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9923379999999999"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_model.fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Тимур.GAMESTATION\\\\vir\\\\Scripts\\\\FEDOT\\\\examples\\\\data\\\\creditcard_under\\\\test.csv'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = 'C:\\\\Users\\\\Тимур.GAMESTATION\\\\vir\\\\Scripts\\\\FEDOT\\\\examples\\\\data\\\\creditcard\\\\creditcard.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-13aadcbeb1d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mroc_auc_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mroc_auc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_model_quality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfitted_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'ROC AUC metric is {roc_auc}, PRECISION is {p}, RECALL is {r}, ACCURACY is {a}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-13aadcbeb1d7>\u001b[0m in \u001b[0;36mvalidate_model_quality\u001b[1;34m(model, data_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m                                   \u001b[0my_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                                   \u001b[0mmulti_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ovo'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                                   average='macro'), 3)\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "def validate_model_quality(model: Chain, data_path: str):\n",
    "    dataset_to_validate = InputData.from_csv(data_path)\n",
    "    predicted_labels = model.predict(dataset_to_validate).predict\n",
    "\n",
    "    roc_auc_valid = round(roc_auc(y_true=test_data.target,\n",
    "                                  y_score=predicted_labels.round(),\n",
    "                                  multi_class='ovo',\n",
    "                                  average='macro'), 3)\n",
    "    \n",
    "    p = precision_score(y_true=test_data.target,y_pred=predicted_labels.round())\n",
    "    r = recall_score(y_true=test_data.target, y_pred=predicted_labels.round())\n",
    "    a = accuracy_score(y_true=test_data.target, y_pred=predicted_labels.round())\n",
    "    \n",
    "    return roc_auc_valid, p, r, a\n",
    "\n",
    "roc_auc, p, r, a = validate_model_quality(fitted_model, test_2)\n",
    "\n",
    "print(f'ROC AUC metric is {roc_auc}, PRECISION is {p}, RECALL is {r}, ACCURACY is {a}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC metric is 0.96\n",
      "PRECISION metric is 0.946236559139785\n",
      "RECALL metric is 0.8979591836734694\n",
      "ACCURACY metric is 0.9238578680203046\n"
     ]
    }
   ],
   "source": [
    "#mlp + xgboost + logit => mlp\n",
    "#roc_auc = validate_model_quality(fitted_model, test_file_path)\n",
    "print(f'ROC AUC metric is {roc_auc[0]}')\n",
    "print(f'PRECISION metric is {roc_auc[1]}')\n",
    "print(f'RECALL metric is {roc_auc[2]}')\n",
    "print(f'ACCURACY metric is {roc_auc[3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC metric is 0.975\n",
      "PRECISION metric is 0.9560439560439561\n",
      "RECALL metric is 0.8877551020408163\n",
      "ACCURACY metric is 0.9238578680203046\n"
     ]
    }
   ],
   "source": [
    "# #mlp + xgboost + logit => mlp\n",
    "# roc_auc = validate_model_quality(fitted_model, test_file_path)\n",
    "# print(f'ROC AUC metric is {roc_auc[0]}')\n",
    "# print(f'PRECISION metric is {roc_auc[1]}')\n",
    "# print(f'RECALL metric is {roc_auc[2]}')\n",
    "# print(f'ACCURACY metric is {roc_auc[3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC metric is 0.978\n",
      "PRECISION metric is 0.967032967032967\n",
      "RECALL metric is 0.8979591836734694\n",
      "ACCURACY metric is 0.934010152284264\n"
     ]
    }
   ],
   "source": [
    "#direct_data_model + logit => rf\n",
    "# print(f'ROC AUC metric is {roc_auc[0]}')\n",
    "# print(f'PRECISION metric is {roc_auc[1]}')\n",
    "# print(f'RECALL metric is {roc_auc[2]}')\n",
    "# print(f'ACCURACY metric is {roc_auc[3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC metric is 0.969\n",
      "PRECISION metric is 0.9263157894736842\n",
      "RECALL metric is 0.8979591836734694\n",
      "ACCURACY metric is 0.9137055837563451\n"
     ]
    }
   ],
   "source": [
    "#Одиночная MLP модель\n",
    "# print(f'ROC AUC metric is {roc_auc[0]}')\n",
    "# print(f'PRECISION metric is {roc_auc[1]}')\n",
    "# print(f'RECALL metric is {roc_auc[2]}')\n",
    "# print(f'ACCURACY metric is {roc_auc[3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_first = r'./creditcard_under.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path, test_file_path = create_multi_clf_examples_from_excel(file_path_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = InputData.from_csv(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = Task(task_type=TaskTypesEnum.classification)\n",
    "dataset_to_compose = InputData.from_csv(train_file_path, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_repo = ModelTypesRepository()\n",
    "available_model_types, _ = models_repo.suitable_model(task_type=task.task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_function = MetricsRepository().metric_by_id(ClassificationMetricsEnum.ROCAUC_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method QualityMetric.get_value_with_penalty of <class 'core.composer.metrics.RocAucMetric'>>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "composer_requirements = GPComposerRequirements(\n",
    "        primary=available_model_types, secondary=available_model_types,\n",
    "        max_lead_time=timedelta(seconds=60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "composer_requirements = GPComposerRequirements(\n",
    "        primary='logit', secondary='rf',\n",
    "        max_lead_time=timedelta(seconds=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "composer_requirements = GPComposerRequirements(\n",
    "    primary=available_model_types,\n",
    "    secondary=available_model_types, max_arity=2,\n",
    "    max_depth=3, pop_size=10, num_of_generations=15,\n",
    "    crossover_prob=0.8, mutation_prob=0.8, max_lead_time=timedelta(seconds=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "composer = GPComposer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logit',\n",
       " 'lda',\n",
       " 'qda',\n",
       " 'dt',\n",
       " 'rf',\n",
       " 'mlp',\n",
       " 'knn',\n",
       " 'svc',\n",
       " 'xgboost',\n",
       " 'bernb',\n",
       " 'direct_data_model',\n",
       " 'pca_data_model']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_model_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in chain assessment during composition: Invalid chain configuration: Chain has incorrect models positions. Continue.\n",
      "Error in chain assessment during composition: Invalid chain configuration: Chain has incorrect models positions. Continue.\n",
      "Error in chain assessment during composition: Expected 2D array, got 1D array instead:\n",
      "array=[].\n",
      "Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.. Continue.\n",
      "Generation num: 0\n",
      "spent time: 0.3 min\n",
      "Best metric is -0.98769\n",
      "Composition time: 0.3 min\n",
      "Algorithm was terminated due to processing time limit\n",
      "GP composition finished\n"
     ]
    }
   ],
   "source": [
    "chain_evo_composed = composer.compose_chain(data=dataset_to_compose,\n",
    "                                                initial_chain=None,\n",
    "                                                composer_requirements=composer_requirements,\n",
    "                                                metrics=metric_function, \n",
    "                                                is_visualise=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain from autoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9892853700379758 0.9971910112359551 0.9010152284263959 0.9998244420549057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9892853700379758"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_chain_from_automl(train_file_path: str, test_file_path: str,\n",
    "                          max_run_time: timedelta = timedelta(minutes=10)):\n",
    "    train_data = InputData.from_csv(train_file_path)\n",
    "    test_data = InputData.from_csv(test_file_path)\n",
    "node_logit.model.external_params = {'max_run_time_sec': max_run_time.seconds}\n",
    "    testing_target = test_data.target\n",
    "\n",
    "    #1 model\n",
    "    chain = Chain()\n",
    "    node_logit = PrimaryNode('logit')\n",
    "    \n",
    "    node_lda = PrimaryNode('lda')\n",
    "    node_rf = SecondaryNode('rf')\n",
    "\n",
    "    node_rf.nodes_from = [node_logit, node_lda]\n",
    "\n",
    "    chain.add_node(node_rf)\n",
    "\n",
    "    chain.fit(train_data)\n",
    "    results = chain.predict(test_data)\n",
    "\n",
    "    roc_auc_value = roc_auc(y_true=testing_target,\n",
    "                            y_score=results.predict)\n",
    "    \n",
    "    \n",
    "    p = precision_score(y_true=testing_target,y_pred=results.predict.round())\n",
    "    r = recall_score(y_true=testing_target, y_pred=results.predict.round())\n",
    "    a = accuracy_score(y_true=testing_target, y_pred=results.predict.round())\n",
    "    \n",
    "    print(roc_auc_value, p, r, a)\n",
    "\n",
    "    return roc_auc_value\n",
    "\n",
    "run_chain_from_automl(train_file_path, test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Тимур\\\\vir\\\\Scripts\\\\Fedot\\\\examples\\\\data\\\\creditcard\\\\train.csv',\n",
       " 'C:\\\\Users\\\\Тимур\\\\vir\\\\Scripts\\\\Fedot\\\\examples\\\\data\\\\creditcard\\\\test.csv')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_path, test_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = InputData.from_csv(train_file_path)\n",
    "test_data = InputData.from_csv(test_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_target = test_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = Chain()\n",
    "node_tpot = PrimaryNode('logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_tpot.model.external_params = {'max_run_time_sec': timedelta(10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_lda = PrimaryNode('lda')\n",
    "node_rf = SecondaryNode('rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_rf.nodes_from = [node_tpot, node_lda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.add_node(node_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OutputData(idx=array([223361., 165061., 238186., ..., 131932., 146867., 121958.]), features=array([[ 1.43352000e+05,  1.95504092e+00, -3.80782711e-01, ...,\n",
       "         4.51682478e-02, -4.71447917e-02,  9.99000000e+00],\n",
       "       [ 1.17173000e+05, -4.00975239e-01, -6.26942769e-01, ...,\n",
       "        -3.70468822e-01, -1.44791686e-01,  4.59000000e+01],\n",
       "       [ 1.49565000e+05,  7.25090164e-02,  8.20565650e-01, ...,\n",
       "         2.06394866e-01,  7.02877702e-02,  1.19900000e+01],\n",
       "       ...,\n",
       "       [ 7.97950000e+04, -1.46608925e-01,  9.92946123e-01, ...,\n",
       "        -1.21139194e-01, -1.96195328e-01,  3.94000000e+00],\n",
       "       [ 8.79310000e+04, -2.94863809e+00,  2.35484929e+00, ...,\n",
       "         4.96912107e-01,  3.35821632e-01,  1.00000000e+00],\n",
       "       [ 7.63810000e+04,  1.23317435e+00, -7.84850501e-01, ...,\n",
       "         1.21657270e-03,  3.85878912e-02,  1.13000000e+02]]), task=Task(task_type=<TaskTypesEnum.classification: ('classification',)>, task_params=None), data_type=<DataTypesEnum.table: 'feature_table'>, predict=array([6.94352513e-05, 6.94352513e-05, 6.94352513e-05, ...,\n",
       "       6.44834432e-04, 6.94352513e-05, 6.94352513e-05]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = chain.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9887529521335848\n"
     ]
    }
   ],
   "source": [
    "roc_auc_value = roc_auc(y_true=testing_target,\n",
    "                        y_score=results.predict)\n",
    "print(roc_auc_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_tuning(nodes_to_tune: str, chain: Chain, train_data: InputData,\n",
    "                 test_data: InputData, local_iter: int,\n",
    "                 tuner_iter_num: int = 50) -> (float, list):\n",
    "    several_iter_scores_test = []\n",
    "\n",
    "    if nodes_to_tune == 'primary':\n",
    "        print('primary_node_tuning')\n",
    "        chain_tune_strategy = chain.fine_tune_primary_nodes\n",
    "    elif nodes_to_tune == 'root':\n",
    "        print('root_node_tuning')\n",
    "        chain_tune_strategy = chain.fine_tune_all_nodes\n",
    "    else:\n",
    "        raise ValueError(f'Invalid type of nodes. Nodes must be primary or root')\n",
    "\n",
    "    for iteration in range(local_iter):\n",
    "        print(f'current local iteration {iteration}')\n",
    "\n",
    "        # Chain tuning\n",
    "        chain_tune_strategy(train_data, iterations=tuner_iter_num)\n",
    "\n",
    "        # After tuning prediction\n",
    "        chain.fit(train_data)\n",
    "        after_tuning_predicted = chain.predict(test_data)\n",
    "\n",
    "        # Metrics\n",
    "        aft_tun_roc_auc = roc_auc(y_true=test_data.target,\n",
    "                                  y_score=after_tuning_predicted.predict)\n",
    "        several_iter_scores_test.append(aft_tun_roc_auc)\n",
    "\n",
    "    return float(np.mean(several_iter_scores_test)), several_iter_scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 model\n",
    "def get_simple_chain():\n",
    "    first = PrimaryNode(model_type='logit')\n",
    "    second = PrimaryNode(model_type='lda')\n",
    "    final = SecondaryNode(model_type='rf',\n",
    "                          nodes_from=[first, second])\n",
    "\n",
    "    chain = Chain(final)\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_first = r'./creditcard.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = 'C:\\\\Users\\\\Тимур\\\\vir\\\\Scripts\\\\Fedot\\\\examples\\\\data\\\\creditcard\\\\train.csv'\n",
    "test_file_path = 'C:\\\\Users\\\\Тимур\\\\vir\\\\Scripts\\\\Fedot\\\\examples\\\\data\\\\creditcard\\\\test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = InputData.from_csv(train_file_path)\n",
    "test_data = InputData.from_csv(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = get_simple_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OutputData(idx=array([223361., 165061., 238186., ..., 131932., 146867., 121958.]), features=array([[ 1.43352000e+05,  1.95504092e+00, -3.80782711e-01, ...,\n",
       "         4.51682478e-02, -4.71447917e-02,  9.99000000e+00],\n",
       "       [ 1.17173000e+05, -4.00975239e-01, -6.26942769e-01, ...,\n",
       "        -3.70468822e-01, -1.44791686e-01,  4.59000000e+01],\n",
       "       [ 1.49565000e+05,  7.25090164e-02,  8.20565650e-01, ...,\n",
       "         2.06394866e-01,  7.02877702e-02,  1.19900000e+01],\n",
       "       ...,\n",
       "       [ 7.97950000e+04, -1.46608925e-01,  9.92946123e-01, ...,\n",
       "        -1.21139194e-01, -1.96195328e-01,  3.94000000e+00],\n",
       "       [ 8.79310000e+04, -2.94863809e+00,  2.35484929e+00, ...,\n",
       "         4.96912107e-01,  3.35821632e-01,  1.00000000e+00],\n",
       "       [ 7.63810000e+04,  1.23317435e+00, -7.84850501e-01, ...,\n",
       "         1.21657270e-03,  3.85878912e-02,  1.13000000e+02]]), task=Task(task_type=<TaskTypesEnum.classification: ('classification',)>, task_params=None), data_type=<DataTypesEnum.table: 'feature_table'>, predict=array([6.96357852e-05, 6.96357852e-05, 6.96357852e-05, ...,\n",
       "       4.78590571e-04, 6.96357852e-05, 6.96357852e-05]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.fit(train_data, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_tuning_predicted = chain.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfr_tun_roc_auc = roc_auc(y_true=test_data.target,\n",
    "                          y_score=before_tuning_predicted.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8283858264\n"
     ]
    }
   ],
   "source": [
    "print(round(bfr_tun_roc_auc, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = precision_score(test_data.target,before_tuning_predicted.predict.round())\n",
    "r = recall_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round())\n",
    "a = accuracy_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7640449438202247, 0.6938775510204082, 0.9991046662687406)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, r, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 model\n",
    "def get_simple_chain():\n",
    "    first = PrimaryNode(model_type='xgboost')\n",
    "    second = PrimaryNode(model_type='lda')\n",
    "    final = SecondaryNode(model_type='rf',\n",
    "                          nodes_from=[first, second])\n",
    "\n",
    "    chain = Chain(final)\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_first = r'./creditcard.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = 'C:\\\\Users\\\\Тимур\\\\vir\\\\Scripts\\\\Fedot\\\\examples\\\\data\\\\creditcard\\\\train.csv'\n",
    "test_file_path = 'C:\\\\Users\\\\Тимур\\\\vir\\\\Scripts\\\\Fedot\\\\examples\\\\data\\\\creditcard\\\\test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = InputData.from_csv(train_file_path)\n",
    "test_data = InputData.from_csv(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = get_simple_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OutputData(idx=array([223361., 165061., 238186., ..., 131932., 146867., 121958.]), features=array([[ 1.43352000e+05,  1.95504092e+00, -3.80782711e-01, ...,\n",
       "         4.51682478e-02, -4.71447917e-02,  9.99000000e+00],\n",
       "       [ 1.17173000e+05, -4.00975239e-01, -6.26942769e-01, ...,\n",
       "        -3.70468822e-01, -1.44791686e-01,  4.59000000e+01],\n",
       "       [ 1.49565000e+05,  7.25090164e-02,  8.20565650e-01, ...,\n",
       "         2.06394866e-01,  7.02877702e-02,  1.19900000e+01],\n",
       "       ...,\n",
       "       [ 7.97950000e+04, -1.46608925e-01,  9.92946123e-01, ...,\n",
       "        -1.21139194e-01, -1.96195328e-01,  3.94000000e+00],\n",
       "       [ 8.79310000e+04, -2.94863809e+00,  2.35484929e+00, ...,\n",
       "         4.96912107e-01,  3.35821632e-01,  1.00000000e+00],\n",
       "       [ 7.63810000e+04,  1.23317435e+00, -7.84850501e-01, ...,\n",
       "         1.21657270e-03,  3.85878912e-02,  1.13000000e+02]]), task=Task(task_type=<TaskTypesEnum.classification: ('classification',)>, task_params=None), data_type=<DataTypesEnum.table: 'feature_table'>, predict=array([0., 0., 0., ..., 0., 0., 0.]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.fit(train_data, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_tuning_predicted = chain.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfr_tun_roc_auc = roc_auc(y_true=test_data.target,\n",
    "                          y_score=before_tuning_predicted.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(round(bfr_tun_roc_auc, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_data.target,before_tuning_predicted.predict.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = precision_score(test_data.target,before_tuning_predicted.predict.round())\n",
    "r = recall_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round())\n",
    "a = accuracy_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0, 1.0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, r, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanced classes to Under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_first = r'./creditcard.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./creditcard.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, full_file_path = create_multi_clf_examples_from_excel(file_path_first, return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = InputData.from_csv(full_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    284315\n",
       "1       492\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Class').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Class'])\n",
    "y = df.iloc[:,[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res, y_res = rus.fit_resample(X, y)\n",
    "X_res['Class'] = y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>82450.0</td>\n",
       "      <td>1.314539</td>\n",
       "      <td>0.590643</td>\n",
       "      <td>-0.666593</td>\n",
       "      <td>0.716564</td>\n",
       "      <td>0.301978</td>\n",
       "      <td>-1.125467</td>\n",
       "      <td>0.388881</td>\n",
       "      <td>-0.288390</td>\n",
       "      <td>-0.132137</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170307</td>\n",
       "      <td>-0.429655</td>\n",
       "      <td>-0.141341</td>\n",
       "      <td>-0.200195</td>\n",
       "      <td>0.639491</td>\n",
       "      <td>0.399476</td>\n",
       "      <td>-0.034321</td>\n",
       "      <td>0.031692</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50554.0</td>\n",
       "      <td>-0.798672</td>\n",
       "      <td>1.185093</td>\n",
       "      <td>0.904547</td>\n",
       "      <td>0.694584</td>\n",
       "      <td>0.219041</td>\n",
       "      <td>-0.319295</td>\n",
       "      <td>0.495236</td>\n",
       "      <td>0.139269</td>\n",
       "      <td>-0.760214</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202287</td>\n",
       "      <td>0.578699</td>\n",
       "      <td>-0.092245</td>\n",
       "      <td>0.013723</td>\n",
       "      <td>-0.246466</td>\n",
       "      <td>-0.380057</td>\n",
       "      <td>-0.396030</td>\n",
       "      <td>-0.112901</td>\n",
       "      <td>4.18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55125.0</td>\n",
       "      <td>-0.391128</td>\n",
       "      <td>-0.245540</td>\n",
       "      <td>1.122074</td>\n",
       "      <td>-1.308725</td>\n",
       "      <td>-0.639891</td>\n",
       "      <td>0.008678</td>\n",
       "      <td>-0.701304</td>\n",
       "      <td>-0.027315</td>\n",
       "      <td>-2.628854</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133485</td>\n",
       "      <td>0.117403</td>\n",
       "      <td>-0.191748</td>\n",
       "      <td>-0.488642</td>\n",
       "      <td>-0.309774</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.163716</td>\n",
       "      <td>0.239582</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>116572.0</td>\n",
       "      <td>-0.060302</td>\n",
       "      <td>1.065093</td>\n",
       "      <td>-0.987421</td>\n",
       "      <td>-0.029567</td>\n",
       "      <td>0.176376</td>\n",
       "      <td>-1.348539</td>\n",
       "      <td>0.775644</td>\n",
       "      <td>0.134843</td>\n",
       "      <td>-0.149734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355576</td>\n",
       "      <td>0.907570</td>\n",
       "      <td>-0.018454</td>\n",
       "      <td>-0.126269</td>\n",
       "      <td>-0.339923</td>\n",
       "      <td>-0.150285</td>\n",
       "      <td>-0.023634</td>\n",
       "      <td>0.042330</td>\n",
       "      <td>57.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90434.0</td>\n",
       "      <td>1.848433</td>\n",
       "      <td>0.373364</td>\n",
       "      <td>0.269272</td>\n",
       "      <td>3.866438</td>\n",
       "      <td>0.088062</td>\n",
       "      <td>0.970447</td>\n",
       "      <td>-0.721945</td>\n",
       "      <td>0.235983</td>\n",
       "      <td>0.683491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103563</td>\n",
       "      <td>0.620954</td>\n",
       "      <td>0.197077</td>\n",
       "      <td>0.692392</td>\n",
       "      <td>-0.206530</td>\n",
       "      <td>-0.021328</td>\n",
       "      <td>-0.019823</td>\n",
       "      <td>-0.042682</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>169142.0</td>\n",
       "      <td>-1.927883</td>\n",
       "      <td>1.125653</td>\n",
       "      <td>-4.518331</td>\n",
       "      <td>1.749293</td>\n",
       "      <td>-1.566487</td>\n",
       "      <td>-2.010494</td>\n",
       "      <td>-0.882850</td>\n",
       "      <td>0.697211</td>\n",
       "      <td>-2.064945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778584</td>\n",
       "      <td>-0.319189</td>\n",
       "      <td>0.639419</td>\n",
       "      <td>-0.294885</td>\n",
       "      <td>0.537503</td>\n",
       "      <td>0.788395</td>\n",
       "      <td>0.292680</td>\n",
       "      <td>0.147968</td>\n",
       "      <td>390.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>169347.0</td>\n",
       "      <td>1.378559</td>\n",
       "      <td>1.289381</td>\n",
       "      <td>-5.004247</td>\n",
       "      <td>1.411850</td>\n",
       "      <td>0.442581</td>\n",
       "      <td>-1.326536</td>\n",
       "      <td>-1.413170</td>\n",
       "      <td>0.248525</td>\n",
       "      <td>-1.127396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370612</td>\n",
       "      <td>0.028234</td>\n",
       "      <td>-0.145640</td>\n",
       "      <td>-0.081049</td>\n",
       "      <td>0.521875</td>\n",
       "      <td>0.739467</td>\n",
       "      <td>0.389152</td>\n",
       "      <td>0.186637</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>169351.0</td>\n",
       "      <td>-0.676143</td>\n",
       "      <td>1.126366</td>\n",
       "      <td>-2.213700</td>\n",
       "      <td>0.468308</td>\n",
       "      <td>-1.120541</td>\n",
       "      <td>-0.003346</td>\n",
       "      <td>-2.234739</td>\n",
       "      <td>1.210158</td>\n",
       "      <td>-0.652250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751826</td>\n",
       "      <td>0.834108</td>\n",
       "      <td>0.190944</td>\n",
       "      <td>0.032070</td>\n",
       "      <td>-0.739695</td>\n",
       "      <td>0.471111</td>\n",
       "      <td>0.385107</td>\n",
       "      <td>0.194361</td>\n",
       "      <td>77.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>169966.0</td>\n",
       "      <td>-3.113832</td>\n",
       "      <td>0.585864</td>\n",
       "      <td>-5.399730</td>\n",
       "      <td>1.817092</td>\n",
       "      <td>-0.840618</td>\n",
       "      <td>-2.943548</td>\n",
       "      <td>-2.208002</td>\n",
       "      <td>1.058733</td>\n",
       "      <td>-1.632333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583276</td>\n",
       "      <td>-0.269209</td>\n",
       "      <td>-0.456108</td>\n",
       "      <td>-0.183659</td>\n",
       "      <td>-0.328168</td>\n",
       "      <td>0.606116</td>\n",
       "      <td>0.884876</td>\n",
       "      <td>-0.253700</td>\n",
       "      <td>245.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>170348.0</td>\n",
       "      <td>1.991976</td>\n",
       "      <td>0.158476</td>\n",
       "      <td>-2.583441</td>\n",
       "      <td>0.408670</td>\n",
       "      <td>1.151147</td>\n",
       "      <td>-0.096695</td>\n",
       "      <td>0.223050</td>\n",
       "      <td>-0.068384</td>\n",
       "      <td>0.577829</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164350</td>\n",
       "      <td>-0.295135</td>\n",
       "      <td>-0.072173</td>\n",
       "      <td>-0.450261</td>\n",
       "      <td>0.313267</td>\n",
       "      <td>-0.289617</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>-0.015309</td>\n",
       "      <td>42.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>984 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0     82450.0  1.314539  0.590643 -0.666593  0.716564  0.301978 -1.125467   \n",
       "1     50554.0 -0.798672  1.185093  0.904547  0.694584  0.219041 -0.319295   \n",
       "2     55125.0 -0.391128 -0.245540  1.122074 -1.308725 -0.639891  0.008678   \n",
       "3    116572.0 -0.060302  1.065093 -0.987421 -0.029567  0.176376 -1.348539   \n",
       "4     90434.0  1.848433  0.373364  0.269272  3.866438  0.088062  0.970447   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "979  169142.0 -1.927883  1.125653 -4.518331  1.749293 -1.566487 -2.010494   \n",
       "980  169347.0  1.378559  1.289381 -5.004247  1.411850  0.442581 -1.326536   \n",
       "981  169351.0 -0.676143  1.126366 -2.213700  0.468308 -1.120541 -0.003346   \n",
       "982  169966.0 -3.113832  0.585864 -5.399730  1.817092 -0.840618 -2.943548   \n",
       "983  170348.0  1.991976  0.158476 -2.583441  0.408670  1.151147 -0.096695   \n",
       "\n",
       "           V7        V8        V9  ...       V21       V22       V23  \\\n",
       "0    0.388881 -0.288390 -0.132137  ... -0.170307 -0.429655 -0.141341   \n",
       "1    0.495236  0.139269 -0.760214  ...  0.202287  0.578699 -0.092245   \n",
       "2   -0.701304 -0.027315 -2.628854  ... -0.133485  0.117403 -0.191748   \n",
       "3    0.775644  0.134843 -0.149734  ...  0.355576  0.907570 -0.018454   \n",
       "4   -0.721945  0.235983  0.683491  ...  0.103563  0.620954  0.197077   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "979 -0.882850  0.697211 -2.064945  ...  0.778584 -0.319189  0.639419   \n",
       "980 -1.413170  0.248525 -1.127396  ...  0.370612  0.028234 -0.145640   \n",
       "981 -2.234739  1.210158 -0.652250  ...  0.751826  0.834108  0.190944   \n",
       "982 -2.208002  1.058733 -1.632333  ...  0.583276 -0.269209 -0.456108   \n",
       "983  0.223050 -0.068384  0.577829  ... -0.164350 -0.295135 -0.072173   \n",
       "\n",
       "          V24       V25       V26       V27       V28  Amount  Class  \n",
       "0   -0.200195  0.639491  0.399476 -0.034321  0.031692    0.76      0  \n",
       "1    0.013723 -0.246466 -0.380057 -0.396030 -0.112901    4.18      0  \n",
       "2   -0.488642 -0.309774  0.008100  0.163716  0.239582   15.00      0  \n",
       "3   -0.126269 -0.339923 -0.150285 -0.023634  0.042330   57.00      0  \n",
       "4    0.692392 -0.206530 -0.021328 -0.019823 -0.042682    0.00      0  \n",
       "..        ...       ...       ...       ...       ...     ...    ...  \n",
       "979 -0.294885  0.537503  0.788395  0.292680  0.147968  390.00      1  \n",
       "980 -0.081049  0.521875  0.739467  0.389152  0.186637    0.76      1  \n",
       "981  0.032070 -0.739695  0.471111  0.385107  0.194361   77.89      1  \n",
       "982 -0.183659 -0.328168  0.606116  0.884876 -0.253700  245.00      1  \n",
       "983 -0.450261  0.313267 -0.289617  0.002988 -0.015309   42.53      1  \n",
       "\n",
       "[984 rows x 31 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res.to_csv(r'.\\creditcard_under.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanced classes to Over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_first = r'./creditcard.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./creditcard.csv'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, full_file_path = create_multi_clf_examples_from_excel(file_path_first, return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = InputData.from_csv(full_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    284315\n",
       "1       492\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Class').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Class'])\n",
    "y = df.iloc[:,[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomOverSampler(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res, y_res = rus.fit_resample(X, y)\n",
    "X_res['Class'] = y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568625</th>\n",
       "      <td>34521.0</td>\n",
       "      <td>1.081234</td>\n",
       "      <td>0.416414</td>\n",
       "      <td>0.862919</td>\n",
       "      <td>2.520863</td>\n",
       "      <td>-0.005021</td>\n",
       "      <td>0.563341</td>\n",
       "      <td>-0.123372</td>\n",
       "      <td>0.223122</td>\n",
       "      <td>-0.673598</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.159387</td>\n",
       "      <td>-0.305154</td>\n",
       "      <td>0.053620</td>\n",
       "      <td>0.011761</td>\n",
       "      <td>0.375146</td>\n",
       "      <td>-0.106299</td>\n",
       "      <td>0.021008</td>\n",
       "      <td>0.010559</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568626</th>\n",
       "      <td>53658.0</td>\n",
       "      <td>-1.739341</td>\n",
       "      <td>1.344521</td>\n",
       "      <td>-0.534379</td>\n",
       "      <td>3.195291</td>\n",
       "      <td>-0.416196</td>\n",
       "      <td>-1.261961</td>\n",
       "      <td>-2.340991</td>\n",
       "      <td>0.713004</td>\n",
       "      <td>-1.416265</td>\n",
       "      <td>...</td>\n",
       "      <td>0.383180</td>\n",
       "      <td>-0.213952</td>\n",
       "      <td>-0.336640</td>\n",
       "      <td>0.237076</td>\n",
       "      <td>0.246003</td>\n",
       "      <td>-0.044228</td>\n",
       "      <td>0.510729</td>\n",
       "      <td>0.220952</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568627</th>\n",
       "      <td>34687.0</td>\n",
       "      <td>-0.860827</td>\n",
       "      <td>3.131790</td>\n",
       "      <td>-5.052968</td>\n",
       "      <td>5.420941</td>\n",
       "      <td>-2.494141</td>\n",
       "      <td>-1.811287</td>\n",
       "      <td>-5.479117</td>\n",
       "      <td>1.189472</td>\n",
       "      <td>-3.908206</td>\n",
       "      <td>...</td>\n",
       "      <td>1.192694</td>\n",
       "      <td>0.090356</td>\n",
       "      <td>-0.341881</td>\n",
       "      <td>-0.215924</td>\n",
       "      <td>1.053032</td>\n",
       "      <td>0.271139</td>\n",
       "      <td>1.373300</td>\n",
       "      <td>0.691195</td>\n",
       "      <td>19.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568628</th>\n",
       "      <td>40276.0</td>\n",
       "      <td>1.159373</td>\n",
       "      <td>2.844795</td>\n",
       "      <td>-4.050680</td>\n",
       "      <td>4.777701</td>\n",
       "      <td>2.948980</td>\n",
       "      <td>-2.010361</td>\n",
       "      <td>1.744086</td>\n",
       "      <td>-0.410287</td>\n",
       "      <td>-2.450198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.176541</td>\n",
       "      <td>-0.433470</td>\n",
       "      <td>-0.529323</td>\n",
       "      <td>-0.597020</td>\n",
       "      <td>1.335954</td>\n",
       "      <td>0.547092</td>\n",
       "      <td>0.009979</td>\n",
       "      <td>0.160769</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568629</th>\n",
       "      <td>102542.0</td>\n",
       "      <td>-1.456876</td>\n",
       "      <td>3.740306</td>\n",
       "      <td>-7.404518</td>\n",
       "      <td>7.440964</td>\n",
       "      <td>-1.549878</td>\n",
       "      <td>-1.661697</td>\n",
       "      <td>-5.757213</td>\n",
       "      <td>1.615011</td>\n",
       "      <td>-2.194881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957897</td>\n",
       "      <td>0.145339</td>\n",
       "      <td>-0.044704</td>\n",
       "      <td>-0.544962</td>\n",
       "      <td>-0.757757</td>\n",
       "      <td>-0.005352</td>\n",
       "      <td>0.318152</td>\n",
       "      <td>-0.323554</td>\n",
       "      <td>2.28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568630 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0            0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1            0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2            1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3            1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4            2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "568625   34521.0  1.081234  0.416414  0.862919  2.520863 -0.005021  0.563341   \n",
       "568626   53658.0 -1.739341  1.344521 -0.534379  3.195291 -0.416196 -1.261961   \n",
       "568627   34687.0 -0.860827  3.131790 -5.052968  5.420941 -2.494141 -1.811287   \n",
       "568628   40276.0  1.159373  2.844795 -4.050680  4.777701  2.948980 -2.010361   \n",
       "568629  102542.0 -1.456876  3.740306 -7.404518  7.440964 -1.549878 -1.661697   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "0       0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474   \n",
       "1      -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288   \n",
       "2       0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412   \n",
       "3       0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321   \n",
       "4       0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "568625 -0.123372  0.223122 -0.673598  ... -0.159387 -0.305154  0.053620   \n",
       "568626 -2.340991  0.713004 -1.416265  ...  0.383180 -0.213952 -0.336640   \n",
       "568627 -5.479117  1.189472 -3.908206  ...  1.192694  0.090356 -0.341881   \n",
       "568628  1.744086 -0.410287 -2.450198  ... -0.176541 -0.433470 -0.529323   \n",
       "568629 -5.757213  1.615011 -2.194881  ...  0.957897  0.145339 -0.044704   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Amount  Class  \n",
       "0       0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1      -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2      -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3      -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4       0.141267 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "...          ...       ...       ...       ...       ...     ...    ...  \n",
       "568625  0.011761  0.375146 -0.106299  0.021008  0.010559    1.52      1  \n",
       "568626  0.237076  0.246003 -0.044228  0.510729  0.220952    0.00      1  \n",
       "568627 -0.215924  1.053032  0.271139  1.373300  0.691195   19.02      1  \n",
       "568628 -0.597020  1.335954  0.547092  0.009979  0.160769    1.00      1  \n",
       "568629 -0.544962 -0.757757 -0.005352  0.318152 -0.323554    2.28      1  \n",
       "\n",
       "[568630 rows x 31 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res.to_csv(r'C:\\Users\\Тимур\\vir\\Scripts\\Fedot\\export_dataframe_over.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on balanced under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_first = r'./export_dataframe.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path, test_file_path = create_multi_clf_examples_from_excel(file_path_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Тимур\\\\vir\\\\Scripts\\\\Fedot\\\\examples\\\\data\\\\export_dataframe\\\\train.csv',\n",
       " 'C:\\\\Users\\\\Тимур\\\\vir\\\\Scripts\\\\Fedot\\\\examples\\\\data\\\\export_dataframe\\\\test.csv')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_path, test_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = InputData.from_csv(train_file_path)\n",
    "test_data = InputData.from_csv(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InputData(idx=array([613., 451., 731., 436., 275., 582., 707., 299., 718., 494., 351.,\n",
       "       594., 652., 865., 294.,  66., 377., 682., 755., 107., 139.,  67.,\n",
       "       832., 213., 670., 486., 425., 363.,  76., 298.,  88.,  59., 420.,\n",
       "       237., 848., 221., 362.,  23.,  30., 477., 266., 168., 589., 281.,\n",
       "       493., 643., 669., 479., 280., 535., 778., 490., 758., 734., 588.,\n",
       "       660., 630., 440., 899., 859., 136.,  39., 359., 244., 296., 355.,\n",
       "       937.,  70., 449., 920., 342., 312., 158., 761., 946., 310., 853.,\n",
       "        63., 813., 199., 593., 516., 712., 928.,  96., 959., 321., 542.,\n",
       "       789., 969., 120.,  86., 570., 547., 878., 558., 533., 668., 689.,\n",
       "       615., 887., 792., 254., 909., 921., 798., 184., 836., 819., 979.,\n",
       "       522.,  55., 746., 247., 869., 260.,  72., 655.,  44., 915., 218.,\n",
       "       618., 286., 694., 567., 823., 807., 314., 215., 678., 635., 519.,\n",
       "       394., 259., 926., 333., 465.,  60., 521., 587., 730., 852., 585.,\n",
       "       370., 500., 290., 198., 305., 306., 110., 448., 467., 323., 664.,\n",
       "       235., 814., 808., 408., 616., 361., 210., 137., 485., 552., 604.,\n",
       "       826., 549., 548., 174., 357., 576., 644., 332.,  78.,  29., 265.,\n",
       "       261., 445., 634., 165., 554., 580., 442.,  65., 141., 432., 381.,\n",
       "       688., 656., 870., 828., 917., 292., 209., 506.,  49., 717.]), features=array([[ 4.12330000e+04, -1.06457996e+01,  5.91830666e+00, ...,\n",
       "         2.73328727e-01, -1.52908081e-01,  0.00000000e+00],\n",
       "       [ 1.30800000e+03, -1.37984835e+00,  5.36719684e-01, ...,\n",
       "        -4.38189291e-01, -3.46730811e-01,  4.38000000e+01],\n",
       "       [ 7.23270000e+04, -4.19873461e+00,  1.94120637e-01, ...,\n",
       "         1.24941367e+00, -1.31524644e-01,  2.38900000e+02],\n",
       "       ...,\n",
       "       [ 8.16900000e+03,  8.57321004e-01,  4.09391183e+00, ...,\n",
       "         6.18323805e-01,  1.48468944e-01,  1.00000000e+00],\n",
       "       [ 3.73140000e+04,  1.43845005e+00, -1.10818642e+00, ...,\n",
       "         2.10431479e-02,  3.25145816e-02,  3.00000000e+01],\n",
       "       [ 6.82070000e+04, -1.31926710e+01,  1.27859706e+01, ...,\n",
       "         1.26956636e+00,  9.39407363e-01,  1.00000000e+00]]), task=Task(task_type=<TaskTypesEnum.classification: ('classification',)>, task_params=None), data_type=<DataTypesEnum.table: 'feature_table'>, target=array([1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
       "       1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
       "       1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 1., 1., 1., 0., 0., 1., 0., 1.]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_chain():\n",
    "    first = PrimaryNode(model_type='logit')\n",
    "    second = PrimaryNode(model_type='lda')\n",
    "    final = SecondaryNode(model_type='rf',\n",
    "                          nodes_from=[first, second])\n",
    "\n",
    "    chain = Chain(final)\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = get_simple_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OutputData(idx=array([962., 762., 334., 889., 529., 468., 978., 231., 568.,  33.,  31.,\n",
       "       736., 192., 318., 495., 933., 309., 934., 715., 109., 411.,  77.,\n",
       "       680., 923.,  82., 404., 331., 605., 382., 539., 845.,   2., 101.,\n",
       "       371., 462., 453., 208., 884.,   5.,  54., 307., 857., 811., 447.,\n",
       "        97., 597., 204., 514., 628., 944., 527., 344., 854.,  25.,  84.,\n",
       "        10., 892., 770., 621., 365., 752., 118., 350., 250., 705., 940.,\n",
       "       196., 596.,  81., 958., 457., 611., 239., 277., 211., 708., 858.,\n",
       "       713., 227., 842., 497., 785., 482., 636., 346., 450., 824., 352.,\n",
       "         7., 155., 534., 422., 704., 380., 767., 559., 319., 591., 919.,\n",
       "       398., 523., 809., 501., 430., 982., 228., 764., 876., 212.,  79.,\n",
       "       148., 302., 545., 536., 327., 973., 799., 133., 543., 311., 599.,\n",
       "       423.,   0., 316., 697., 797., 328., 525., 970., 433., 172., 125.,\n",
       "       541., 879., 888.,  90., 924., 181., 274., 880.,  69., 291., 131.,\n",
       "       300., 802., 326., 144., 405., 787., 135., 532., 164.,  28., 783.,\n",
       "       193., 901., 629., 169., 728., 140., 173.,   6., 633., 638.,  73.,\n",
       "       849., 861., 238., 145., 781., 234., 220., 456., 481., 132., 974.,\n",
       "       528., 185.,  41., 602., 108., 581.,  56., 388., 424., 788., 846.,\n",
       "        24., 428., 790., 687., 685., 911., 338.,  51., 696., 931., 905.,\n",
       "       662., 264., 530., 673., 483., 507., 429.,  18., 706., 464., 903.,\n",
       "       722., 367.,  83.,  61., 439., 272., 285., 360., 354., 720., 278.,\n",
       "        12., 182., 368., 518., 753., 223., 851., 810., 803., 499., 575.,\n",
       "       601., 176., 657., 784., 578., 513., 786., 163., 248., 626., 902.,\n",
       "       963., 375., 412.,  74., 113., 631., 863., 390., 104., 114., 417.,\n",
       "       910., 409.,  92., 557.,  89., 336., 972., 906., 918., 598., 756.,\n",
       "       603.,  94.,  11., 396., 526.,  43.,  42., 329., 167., 478., 886.,\n",
       "       590., 745., 100., 426., 178., 444., 416., 692., 868., 650., 177.,\n",
       "       395., 667., 939., 945., 675., 383., 941., 257., 531., 335., 606.,\n",
       "        15.,   3., 867., 908., 256., 538., 948., 737., 572., 393., 222.,\n",
       "       179., 289., 967., 324., 583.,   9., 249.,  22., 356., 877., 847.,\n",
       "       340., 431., 544., 820., 203., 622.,  93., 551.,  68., 975., 284.,\n",
       "       872., 434., 153.,  75., 721., 446., 188., 271., 236., 487., 117.,\n",
       "       957., 512., 620., 584., 126., 116., 473., 684.,  57., 665., 914.,\n",
       "       369., 268.,  46., 349., 195., 983., 895., 881., 263., 443., 666.,\n",
       "       304., 341., 951., 149., 124., 907.,  50., 353., 912., 142., 470.,\n",
       "       399., 617., 320.,  19., 796., 981., 795., 407., 537., 740.,  38.,\n",
       "       175., 245., 885., 893., 743., 844., 154., 287., 595., 569., 732.,\n",
       "        17., 127., 322., 255., 649., 949., 190., 115., 625., 180., 301.,\n",
       "       800., 703., 714., 760., 653., 517., 968.,  45., 894., 157., 932.,\n",
       "       171.,  16., 511.,  48., 955., 827., 515., 677., 480., 283., 723.,\n",
       "       927., 225.,  26., 896., 437., 936., 364., 229.,  37., 950., 374.,\n",
       "       469., 952., 837., 695., 716., 194., 925., 850., 503., 954., 817.,\n",
       "       579., 953., 162., 693., 152., 672., 750., 822., 111., 226., 679.,\n",
       "       103., 421., 419., 739., 586., 961., 119.,  53., 151., 403., 930.,\n",
       "       207., 658., 830., 751.,   8., 816.,  36., 452., 651., 253., 303.,\n",
       "       735., 571., 623., 977., 710., 262., 610., 297., 414., 150., 777.,\n",
       "       640., 874., 550., 780., 488., 147., 146., 711., 916., 891., 659.,\n",
       "       348., 463., 325., 186., 123., 839., 608., 143., 943., 197., 609.,\n",
       "       279., 293., 400., 122., 183., 202., 438., 246., 415., 765., 754.,\n",
       "       757., 873., 129., 637., 402., 773., 759., 898., 219., 641., 900.,\n",
       "       741., 793., 904., 624., 825., 749., 386., 956., 509., 267., 806.,\n",
       "       441., 496., 112., 691., 232., 855., 607., 671., 373., 965., 829.,\n",
       "       233., 774., 676., 317., 648., 410., 883., 709., 358., 258., 744.,\n",
       "       627., 632., 282., 376., 384., 224., 938., 801., 472., 347., 505.,\n",
       "       639., 971., 913., 890., 619., 841., 645., 833., 556., 942., 577.,\n",
       "        85., 242., 698., 159., 524.,  35., 540., 170., 654., 935., 843.,\n",
       "       834., 929., 733.,  95., 563., 240., 742., 574., 690., 460., 553.,\n",
       "       864., 206., 392., 794., 397., 766., 835., 217.,   4., 768., 642.,\n",
       "       882., 612., 738., 546., 725., 683.,  98., 804., 727., 573., 406.,\n",
       "       502.,  47.,  32., 779., 200., 134.,  27., 866., 230., 489., 772.,\n",
       "       378., 288., 418., 674., 391., 592., 498., 138.,  62., 471., 647.,\n",
       "       128., 960., 520., 838., 947.,  64., 812.,  14., 156.,  40., 492.,\n",
       "       379., 187., 763., 216., 791.,  52., 337., 748., 719., 724., 295.,\n",
       "       701., 251., 726., 461., 455., 980., 815., 862., 269., 201., 161.,\n",
       "       555., 729., 401., 702., 476., 821., 771., 105., 565., 389.,   1.,\n",
       "       922., 966., 561.,  80., 205.,  34., 775., 508., 427., 454., 366.,\n",
       "        91., 339., 897., 564., 345., 776., 241.,  13., 315., 600., 387.,\n",
       "       273., 166., 840., 976., 646., 818., 484., 964., 504., 831., 243.,\n",
       "       566., 875., 562., 686., 189., 782., 699., 475., 681., 510.,  58.,\n",
       "       474., 560., 856., 747., 252.,  21., 313., 459., 160., 276., 191.,\n",
       "       385., 805., 413., 491., 343., 769., 308., 661., 130., 663., 871.,\n",
       "        99., 372.,  87., 458., 330., 214., 466., 121., 614.,  20., 700.,\n",
       "        71., 106., 270., 860., 435., 102.]), features=array([[ 1.58638000e+05, -5.97611932e+00, -7.19697963e+00, ...,\n",
       "         5.65846302e-01, -1.03410719e+00,  2.96000000e+02],\n",
       "       [ 8.47890000e+04, -1.43086373e+00, -8.02528699e-01, ...,\n",
       "        -1.20351073e-01,  3.55941734e-02,  3.54330000e+02],\n",
       "       [ 1.45056000e+05, -2.71359479e-01,  1.29843749e+00, ...,\n",
       "         1.33778090e-01,  1.70118018e-01,  4.91600000e+01],\n",
       "       ...,\n",
       "       [ 1.28471000e+05,  9.09123839e-01,  1.33765782e+00, ...,\n",
       "         6.52673807e-01,  3.19879228e-01,  6.79000000e+01],\n",
       "       [ 1.22517000e+05,  1.55478327e+00, -1.05940877e+00, ...,\n",
       "        -3.55714131e-02, -8.57818648e-03,  2.29880000e+02],\n",
       "       [ 8.13240000e+04, -2.37304316e+00,  2.56343777e+00, ...,\n",
       "        -9.07652241e-01, -1.48522812e-01,  2.41800000e+01]]), task=Task(task_type=<TaskTypesEnum.classification: ('classification',)>, task_params=None), data_type=<DataTypesEnum.table: 'feature_table'>, predict=array([1.  , 0.76, 0.27, 1.  , 1.  , 0.  , 1.  , 0.01, 1.  , 0.  , 0.07,\n",
       "       0.67, 0.  , 0.  , 1.  , 1.  , 0.2 , 1.  , 0.6 , 0.  , 0.  , 0.  ,\n",
       "       1.  , 1.  , 0.  , 0.  , 0.  , 1.  , 0.  , 1.  , 1.  , 0.  , 0.  ,\n",
       "       0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.07, 1.  , 1.  , 0.  ,\n",
       "       0.04, 1.  , 0.1 , 1.  , 1.  , 1.  , 1.  , 0.  , 0.99, 0.  , 0.36,\n",
       "       0.  , 1.  , 1.  , 1.  , 0.01, 1.  , 0.26, 0.  , 0.  , 1.  , 0.77,\n",
       "       0.06, 1.  , 0.  , 0.69, 0.  , 1.  , 0.  , 0.  , 0.  , 1.  , 1.  ,\n",
       "       1.  , 0.  , 1.  , 1.  , 1.  , 0.  , 1.  , 0.02, 0.  , 1.  , 0.  ,\n",
       "       0.  , 0.  , 1.  , 0.  , 1.  , 0.  , 1.  , 1.  , 0.  , 1.  , 1.  ,\n",
       "       0.  , 1.  , 1.  , 1.  , 0.03, 1.  , 0.  , 1.  , 1.  , 0.  , 0.  ,\n",
       "       0.01, 0.23, 1.  , 1.  , 0.07, 1.  , 1.  , 0.  , 1.  , 0.14, 1.  ,\n",
       "       0.  , 0.01, 0.  , 0.99, 1.  , 0.02, 1.  , 0.97, 0.  , 0.03, 0.  ,\n",
       "       1.  , 1.  , 0.99, 0.01, 1.  , 0.06, 0.06, 1.  , 0.  , 0.03, 0.  ,\n",
       "       0.  , 1.  , 0.  , 0.  , 0.  , 1.  , 0.14, 0.97, 0.  , 0.01, 1.  ,\n",
       "       0.  , 1.  , 1.  , 0.13, 1.  , 0.  , 0.  , 0.  , 1.  , 1.  , 0.  ,\n",
       "       1.  , 1.  , 0.  , 0.  , 1.  , 0.13, 0.  , 0.06, 0.  , 0.11, 0.86,\n",
       "       1.  , 0.  , 0.07, 1.  , 0.1 , 0.59, 0.  , 0.  , 0.  , 1.  , 1.  ,\n",
       "       0.  , 0.03, 1.  , 1.  , 1.  , 1.  , 0.  , 0.  , 1.  , 1.  , 1.  ,\n",
       "       1.  , 0.  , 1.  , 1.  , 0.03, 1.  , 0.  , 0.  , 1.  , 0.  , 1.  ,\n",
       "       1.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.14, 0.  , 0.  , 1.  , 0.19,\n",
       "       0.  , 0.01, 0.  , 1.  , 0.89, 0.  , 1.  , 1.  , 1.  , 1.  , 1.  ,\n",
       "       1.  , 0.  , 1.  , 1.  , 1.  , 1.  , 1.  , 0.03, 0.04, 1.  , 0.7 ,\n",
       "       1.  , 0.12, 0.  , 0.  , 0.  , 1.  , 1.  , 0.  , 0.11, 0.  , 0.01,\n",
       "       1.  , 0.  , 0.  , 1.  , 0.09, 0.  , 0.98, 1.  , 1.  , 1.  , 1.  ,\n",
       "       1.  , 0.  , 0.  , 0.02, 1.  , 0.02, 0.  , 0.  , 0.05, 0.  , 0.97,\n",
       "       1.  , 1.  , 0.02, 0.  , 0.  , 0.01, 0.03, 1.  , 1.  , 1.  , 0.  ,\n",
       "       0.  , 0.99, 0.79, 1.  , 1.  , 0.04, 0.99, 0.05, 0.9 , 0.  , 1.  ,\n",
       "       0.  , 0.13, 1.  , 1.  , 0.01, 1.  , 1.  , 0.88, 1.  , 0.28, 0.  ,\n",
       "       0.02, 0.  , 1.  , 0.  , 1.  , 0.  , 0.  , 0.03, 0.  , 1.  , 0.88,\n",
       "       0.  , 0.02, 1.  , 1.  , 0.01, 1.  , 0.  , 1.  , 0.  , 1.  , 0.  ,\n",
       "       0.8 , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.01, 0.03, 0.27,\n",
       "       1.  , 1.  , 1.  , 1.  , 0.  , 0.  , 0.  , 1.  , 0.02, 1.  , 1.  ,\n",
       "       0.01, 0.  , 0.  , 0.  , 0.  , 0.85, 1.  , 1.  , 0.01, 0.  , 1.  ,\n",
       "       0.  , 0.  , 1.  , 0.  , 0.  , 1.  , 0.01, 0.  , 1.  , 0.  , 0.  ,\n",
       "       0.  , 1.  , 0.04, 0.  , 1.  , 0.98, 1.  , 0.02, 1.  , 1.  , 0.03,\n",
       "       0.  , 0.18, 1.  , 1.  , 1.  , 0.69, 0.  , 0.  , 1.  , 1.  , 0.74,\n",
       "       0.  , 0.  , 0.  , 0.07, 0.95, 1.  , 0.  , 0.06, 1.  , 0.01, 0.06,\n",
       "       1.  , 0.64, 1.  , 1.  , 0.92, 1.  , 0.91, 0.  , 1.  , 0.  , 1.  ,\n",
       "       0.  , 0.  , 1.  , 0.01, 1.  , 1.  , 1.  , 1.  , 0.  , 0.  , 0.55,\n",
       "       1.  , 0.  , 0.  , 0.65, 0.  , 1.  , 0.  , 0.  , 0.  , 1.  , 0.  ,\n",
       "       0.  , 1.  , 1.  , 1.  , 0.61, 0.03, 1.  , 1.  , 1.  , 1.  , 1.  ,\n",
       "       1.  , 1.  , 0.02, 1.  , 0.01, 1.  , 1.  , 1.  , 0.07, 0.01, 1.  ,\n",
       "       0.07, 0.  , 0.  , 1.  , 1.  , 1.  , 0.  , 0.  , 0.  , 0.06, 1.  ,\n",
       "       0.01, 1.  , 1.  , 1.  , 0.01, 1.  , 0.  , 0.1 , 0.99, 0.  , 0.04,\n",
       "       1.  , 1.  , 1.  , 0.69, 1.  , 0.07, 1.  , 0.  , 0.  , 0.01, 1.  ,\n",
       "       1.  , 1.  , 1.  , 1.  , 0.  , 0.12, 0.  , 0.63, 1.  , 0.8 , 1.  ,\n",
       "       0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 1.  , 0.  , 1.  , 0.04, 1.  ,\n",
       "       0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.99,\n",
       "       1.  , 1.  , 0.  , 1.  , 0.  , 1.  , 1.  , 1.  , 0.  , 0.71, 1.  ,\n",
       "       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 0.03, 1.  , 1.  , 0.  , 1.  ,\n",
       "       0.  , 1.  , 0.  , 1.  , 0.  , 1.  , 1.  , 1.  , 0.  , 1.  , 1.  ,\n",
       "       0.09, 1.  , 1.  , 0.  , 0.86, 0.  , 0.61, 0.67, 0.  , 0.  , 1.  ,\n",
       "       1.  , 1.  , 0.  , 0.19, 0.11, 0.38, 1.  , 1.  , 0.  , 0.02, 1.  ,\n",
       "       0.99, 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ,\n",
       "       0.  , 0.  , 1.  , 0.  , 1.  , 0.01, 1.  , 0.  , 1.  , 1.  , 1.  ,\n",
       "       1.  , 1.  , 1.  , 0.  , 1.  , 0.03, 1.  , 1.  , 1.  , 0.  , 1.  ,\n",
       "       0.9 , 0.  , 0.  , 1.  , 0.02, 1.  , 1.  , 0.16, 0.11, 1.  , 1.  ,\n",
       "       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 0.01, 1.  , 1.  , 1.  , 0.07,\n",
       "       1.  , 0.13, 0.  , 1.  , 0.  , 0.  , 0.  , 1.  , 0.11, 0.07, 1.  ,\n",
       "       0.04, 0.  , 0.  , 0.95, 0.11, 1.  , 1.  , 0.  , 0.  , 0.  , 0.78,\n",
       "       0.23, 1.  , 1.  , 1.  , 1.  , 0.  , 1.  , 0.  , 0.  , 0.  , 1.  ,\n",
       "       0.  , 0.  , 1.  , 0.  , 1.  , 0.  , 0.01, 1.  , 1.  , 1.  , 0.  ,\n",
       "       1.  , 0.  , 1.  , 0.04, 0.  , 1.  , 1.  , 1.  , 0.  , 0.  , 0.  ,\n",
       "       1.  , 0.86, 0.  , 1.  , 0.34, 1.  , 1.  , 0.  , 1.  , 0.  , 0.01,\n",
       "       1.  , 1.  , 1.  , 0.02, 0.04, 0.  , 1.  , 1.  , 0.  , 0.01, 0.05,\n",
       "       0.  , 0.02, 1.  , 1.  , 0.  , 0.99, 0.  , 0.  , 0.  , 1.  , 0.  ,\n",
       "       0.  , 0.04, 1.  , 0.57, 1.  , 1.  , 0.  , 1.  , 1.  , 1.  , 0.  ,\n",
       "       1.  , 1.  , 1.  , 0.94, 0.01, 1.  , 1.  , 0.  , 1.  , 1.  , 0.02,\n",
       "       0.09, 1.  , 1.  , 0.64, 0.  , 0.  , 0.01, 0.  , 0.03, 0.01, 0.  ,\n",
       "       0.  , 1.  , 0.  , 0.1 , 0.  , 1.  , 0.  , 0.74, 0.  , 1.  , 1.  ,\n",
       "       0.  , 0.14, 0.11, 0.  , 0.  , 0.09, 0.03, 0.  , 1.  , 0.  , 1.  ,\n",
       "       0.  , 0.  , 0.  , 1.  , 0.  , 0.  ]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.fit(train_data, use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_tuning_predicted = chain.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9585137085\n"
     ]
    }
   ],
   "source": [
    "bfr_tun_roc_auc = roc_auc(y_true=test_data.target,\n",
    "                          y_score=before_tuning_predicted.predict)\n",
    "print(round(bfr_tun_roc_auc, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9456521739130435, 0.8877551020408163, 0.9187817258883249)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = precision_score(test_data.target,before_tuning_predicted.predict.round())\n",
    "r = recall_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round())\n",
    "a = accuracy_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round())\n",
    "p, r, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8183704334294213 0.7586206896551724 0.673469387755102 0.9990695551420246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8183704334294213"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_chain_from_automl(train_file_path: str, test_file_path: str,\n",
    "                          max_run_time: timedelta = timedelta(minutes=10)):\n",
    "    train_data = InputData.from_csv(train_file_path)\n",
    "    test_data = InputData.from_csv(test_file_path)\n",
    "    \n",
    "    \n",
    "    testing_target = test_data.target\n",
    "\n",
    "    #1 model\n",
    "    chain = Chain()\n",
    "    node_logit = PrimaryNode('logit')\n",
    "    node_logit.model.external_params = {'max_run_time_sec': max_run_time.seconds}\n",
    "    \n",
    "    node_lda = PrimaryNode('lda')\n",
    "    node_rf = SecondaryNode('rf')\n",
    "\n",
    "    node_rf.nodes_from = [node_logit, node_lda]\n",
    "\n",
    "    chain.add_node(node_rf)\n",
    "\n",
    "    chain.fit(train_data)\n",
    "    results = chain.predict(test_data)\n",
    "\n",
    "    roc_auc_value = roc_auc(y_true=testing_target,\n",
    "                            y_score=results.predict)\n",
    "    \n",
    "    \n",
    "    p = precision_score(y_true=testing_target,y_pred=results.predict.round())\n",
    "    r = recall_score(y_true=testing_target, y_pred=results.predict.round())\n",
    "    a = accuracy_score(y_true=testing_target, y_pred=results.predict.round())\n",
    "    \n",
    "    print(roc_auc_value, p, r, a)\n",
    "\n",
    "    return roc_auc_value\n",
    "\n",
    "run_chain_from_automl(train_file_path, test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on balanced over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_first = r'./export_dataframe_over.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path, test_file_path = create_multi_clf_examples_from_excel(file_path_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Тимур\\\\vir\\\\Scripts\\\\Fedot\\\\examples\\\\data\\\\export_dataframe_over\\\\train.csv',\n",
       " 'C:\\\\Users\\\\Тимур\\\\vir\\\\Scripts\\\\Fedot\\\\examples\\\\data\\\\export_dataframe_over\\\\test.csv')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_path, test_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = InputData.from_csv(train_file_path)\n",
    "test_data = InputData.from_csv(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_chain():\n",
    "    first = PrimaryNode(model_type='logit')\n",
    "    second = PrimaryNode(model_type='lda')\n",
    "    final = SecondaryNode(model_type='rf',\n",
    "                          nodes_from=[first, second])\n",
    "\n",
    "    chain = Chain(final)\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = get_simple_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OutputData(idx=array([ 93824.,  39017., 150139., ...,  62341.,  79795.,  76381.]), features=array([[-3.63280895e+00,  5.43726336e+00, -9.13652148e+00, ...,\n",
       "         1.69360751e+00,  8.57685372e-01,  8.54000000e+00],\n",
       "       [-5.86021027e-01, -3.19820450e-02,  2.15357484e+00, ...,\n",
       "        -1.85795736e-01, -1.29465135e-01,  2.54100000e+01],\n",
       "       [-6.68283192e+00, -2.71426804e+00, -5.77453046e+00, ...,\n",
       "        -5.49876280e-02,  8.23370925e-02,  2.37260000e+02],\n",
       "       ...,\n",
       "       [-5.26775974e+00,  2.50671896e+00, -5.29092482e+00, ...,\n",
       "        -1.48676592e+00,  6.77664105e-01,  1.10000000e+00],\n",
       "       [-1.46608925e-01,  9.92946123e-01,  1.52459137e+00, ...,\n",
       "        -1.21139194e-01, -1.96195328e-01,  3.94000000e+00],\n",
       "       [ 1.23317435e+00, -7.84850501e-01,  3.86783869e-01, ...,\n",
       "         1.21657270e-03,  3.85878912e-02,  1.13000000e+02]]), task=Task(task_type=<TaskTypesEnum.classification: ('classification',)>, task_params=None), data_type=<DataTypesEnum.table: 'feature_table'>, predict=array([0.99995802, 0.        , 1.        , ..., 0.99994947, 0.        ,\n",
       "       0.        ]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.fit(train_data, use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_tuning_predicted = chain.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999537225\n"
     ]
    }
   ],
   "source": [
    "bfr_tun_roc_auc = roc_auc(y_true=test_data.target,\n",
    "                          y_score=before_tuning_predicted.predict)\n",
    "print(round(bfr_tun_roc_auc, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9995781569677923, 1.0, 0.9997889664632538)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = precision_score(test_data.target,before_tuning_predicted.predict.round())\n",
    "r = recall_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round())\n",
    "a = accuracy_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round())\n",
    "p, r, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999577180524802 0.9995641016627412 1.0 0.9997819320120289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999577180524802"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_chain_from_automl(train_file_path: str, test_file_path: str,\n",
    "                          max_run_time: timedelta = timedelta(minutes=10)):\n",
    "    train_data = InputData.from_csv(train_file_path)\n",
    "    test_data = InputData.from_csv(test_file_path)\n",
    "    \n",
    "    \n",
    "    testing_target = test_data.target\n",
    "\n",
    "    #1 model\n",
    "    chain = Chain()\n",
    "    node_logit = PrimaryNode('logit')\n",
    "    node_logit.model.external_params = {'max_run_time_sec': max_run_time.seconds}\n",
    "    \n",
    "    node_lda = PrimaryNode('lda')\n",
    "    node_rf = SecondaryNode('rf')\n",
    "\n",
    "    node_rf.nodes_from = [node_logit, node_lda]\n",
    "\n",
    "    chain.add_node(node_rf)\n",
    "\n",
    "    chain.fit(train_data)\n",
    "    results = chain.predict(test_data)\n",
    "\n",
    "    roc_auc_value = roc_auc(y_true=testing_target,\n",
    "                            y_score=results.predict)\n",
    "    \n",
    "    \n",
    "    p = precision_score(y_true=testing_target,y_pred=results.predict.round())\n",
    "    r = recall_score(y_true=testing_target, y_pred=results.predict.round())\n",
    "    a = accuracy_score(y_true=testing_target, y_pred=results.predict.round())\n",
    "    \n",
    "    print(roc_auc_value, p, r, a)\n",
    "\n",
    "    return roc_auc_value\n",
    "\n",
    "run_chain_from_automl(train_file_path, test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on balanced predict on full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_first = r'./export_dataframe.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path, test_file_path = create_multi_clf_examples_from_excel(file_path_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Тимур\\\\vir\\\\Scripts\\\\Fedot\\\\examples\\\\data\\\\export_dataframe\\\\train.csv',\n",
       " 'C:\\\\Users\\\\Тимур\\\\vir\\\\Scripts\\\\Fedot\\\\examples\\\\data\\\\export_dataframe\\\\test.csv')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_path, test_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data1 = pd.read_csv('creditcard.csv')\n",
    "data1.to_csv(r'C:\\Users\\Тимур\\vir\\Scripts\\Fedot\\creditcard_with_index.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path = r'./creditcard_with_index.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = InputData.from_csv(train_file_path)\n",
    "test_data = InputData.from_csv(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_chain():\n",
    "    first = PrimaryNode(model_type='logit')\n",
    "    second = PrimaryNode(model_type='lda')\n",
    "    final = SecondaryNode(model_type='rf',\n",
    "                          nodes_from=[first, second])\n",
    "\n",
    "    chain = Chain(final)\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = get_simple_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OutputData(idx=array([962., 762., 334., 889., 529., 468., 978., 231., 568.,  33.,  31.,\n",
       "       736., 192., 318., 495., 933., 309., 934., 715., 109., 411.,  77.,\n",
       "       680., 923.,  82., 404., 331., 605., 382., 539., 845.,   2., 101.,\n",
       "       371., 462., 453., 208., 884.,   5.,  54., 307., 857., 811., 447.,\n",
       "        97., 597., 204., 514., 628., 944., 527., 344., 854.,  25.,  84.,\n",
       "        10., 892., 770., 621., 365., 752., 118., 350., 250., 705., 940.,\n",
       "       196., 596.,  81., 958., 457., 611., 239., 277., 211., 708., 858.,\n",
       "       713., 227., 842., 497., 785., 482., 636., 346., 450., 824., 352.,\n",
       "         7., 155., 534., 422., 704., 380., 767., 559., 319., 591., 919.,\n",
       "       398., 523., 809., 501., 430., 982., 228., 764., 876., 212.,  79.,\n",
       "       148., 302., 545., 536., 327., 973., 799., 133., 543., 311., 599.,\n",
       "       423.,   0., 316., 697., 797., 328., 525., 970., 433., 172., 125.,\n",
       "       541., 879., 888.,  90., 924., 181., 274., 880.,  69., 291., 131.,\n",
       "       300., 802., 326., 144., 405., 787., 135., 532., 164.,  28., 783.,\n",
       "       193., 901., 629., 169., 728., 140., 173.,   6., 633., 638.,  73.,\n",
       "       849., 861., 238., 145., 781., 234., 220., 456., 481., 132., 974.,\n",
       "       528., 185.,  41., 602., 108., 581.,  56., 388., 424., 788., 846.,\n",
       "        24., 428., 790., 687., 685., 911., 338.,  51., 696., 931., 905.,\n",
       "       662., 264., 530., 673., 483., 507., 429.,  18., 706., 464., 903.,\n",
       "       722., 367.,  83.,  61., 439., 272., 285., 360., 354., 720., 278.,\n",
       "        12., 182., 368., 518., 753., 223., 851., 810., 803., 499., 575.,\n",
       "       601., 176., 657., 784., 578., 513., 786., 163., 248., 626., 902.,\n",
       "       963., 375., 412.,  74., 113., 631., 863., 390., 104., 114., 417.,\n",
       "       910., 409.,  92., 557.,  89., 336., 972., 906., 918., 598., 756.,\n",
       "       603.,  94.,  11., 396., 526.,  43.,  42., 329., 167., 478., 886.,\n",
       "       590., 745., 100., 426., 178., 444., 416., 692., 868., 650., 177.,\n",
       "       395., 667., 939., 945., 675., 383., 941., 257., 531., 335., 606.,\n",
       "        15.,   3., 867., 908., 256., 538., 948., 737., 572., 393., 222.,\n",
       "       179., 289., 967., 324., 583.,   9., 249.,  22., 356., 877., 847.,\n",
       "       340., 431., 544., 820., 203., 622.,  93., 551.,  68., 975., 284.,\n",
       "       872., 434., 153.,  75., 721., 446., 188., 271., 236., 487., 117.,\n",
       "       957., 512., 620., 584., 126., 116., 473., 684.,  57., 665., 914.,\n",
       "       369., 268.,  46., 349., 195., 983., 895., 881., 263., 443., 666.,\n",
       "       304., 341., 951., 149., 124., 907.,  50., 353., 912., 142., 470.,\n",
       "       399., 617., 320.,  19., 796., 981., 795., 407., 537., 740.,  38.,\n",
       "       175., 245., 885., 893., 743., 844., 154., 287., 595., 569., 732.,\n",
       "        17., 127., 322., 255., 649., 949., 190., 115., 625., 180., 301.,\n",
       "       800., 703., 714., 760., 653., 517., 968.,  45., 894., 157., 932.,\n",
       "       171.,  16., 511.,  48., 955., 827., 515., 677., 480., 283., 723.,\n",
       "       927., 225.,  26., 896., 437., 936., 364., 229.,  37., 950., 374.,\n",
       "       469., 952., 837., 695., 716., 194., 925., 850., 503., 954., 817.,\n",
       "       579., 953., 162., 693., 152., 672., 750., 822., 111., 226., 679.,\n",
       "       103., 421., 419., 739., 586., 961., 119.,  53., 151., 403., 930.,\n",
       "       207., 658., 830., 751.,   8., 816.,  36., 452., 651., 253., 303.,\n",
       "       735., 571., 623., 977., 710., 262., 610., 297., 414., 150., 777.,\n",
       "       640., 874., 550., 780., 488., 147., 146., 711., 916., 891., 659.,\n",
       "       348., 463., 325., 186., 123., 839., 608., 143., 943., 197., 609.,\n",
       "       279., 293., 400., 122., 183., 202., 438., 246., 415., 765., 754.,\n",
       "       757., 873., 129., 637., 402., 773., 759., 898., 219., 641., 900.,\n",
       "       741., 793., 904., 624., 825., 749., 386., 956., 509., 267., 806.,\n",
       "       441., 496., 112., 691., 232., 855., 607., 671., 373., 965., 829.,\n",
       "       233., 774., 676., 317., 648., 410., 883., 709., 358., 258., 744.,\n",
       "       627., 632., 282., 376., 384., 224., 938., 801., 472., 347., 505.,\n",
       "       639., 971., 913., 890., 619., 841., 645., 833., 556., 942., 577.,\n",
       "        85., 242., 698., 159., 524.,  35., 540., 170., 654., 935., 843.,\n",
       "       834., 929., 733.,  95., 563., 240., 742., 574., 690., 460., 553.,\n",
       "       864., 206., 392., 794., 397., 766., 835., 217.,   4., 768., 642.,\n",
       "       882., 612., 738., 546., 725., 683.,  98., 804., 727., 573., 406.,\n",
       "       502.,  47.,  32., 779., 200., 134.,  27., 866., 230., 489., 772.,\n",
       "       378., 288., 418., 674., 391., 592., 498., 138.,  62., 471., 647.,\n",
       "       128., 960., 520., 838., 947.,  64., 812.,  14., 156.,  40., 492.,\n",
       "       379., 187., 763., 216., 791.,  52., 337., 748., 719., 724., 295.,\n",
       "       701., 251., 726., 461., 455., 980., 815., 862., 269., 201., 161.,\n",
       "       555., 729., 401., 702., 476., 821., 771., 105., 565., 389.,   1.,\n",
       "       922., 966., 561.,  80., 205.,  34., 775., 508., 427., 454., 366.,\n",
       "        91., 339., 897., 564., 345., 776., 241.,  13., 315., 600., 387.,\n",
       "       273., 166., 840., 976., 646., 818., 484., 964., 504., 831., 243.,\n",
       "       566., 875., 562., 686., 189., 782., 699., 475., 681., 510.,  58.,\n",
       "       474., 560., 856., 747., 252.,  21., 313., 459., 160., 276., 191.,\n",
       "       385., 805., 413., 491., 343., 769., 308., 661., 130., 663., 871.,\n",
       "        99., 372.,  87., 458., 330., 214., 466., 121., 614.,  20., 700.,\n",
       "        71., 106., 270., 860., 435., 102.]), features=array([[ 1.58638000e+05, -5.97611932e+00, -7.19697963e+00, ...,\n",
       "         5.65846302e-01, -1.03410719e+00,  2.96000000e+02],\n",
       "       [ 8.47890000e+04, -1.43086373e+00, -8.02528699e-01, ...,\n",
       "        -1.20351073e-01,  3.55941734e-02,  3.54330000e+02],\n",
       "       [ 1.45056000e+05, -2.71359479e-01,  1.29843749e+00, ...,\n",
       "         1.33778090e-01,  1.70118018e-01,  4.91600000e+01],\n",
       "       ...,\n",
       "       [ 1.28471000e+05,  9.09123839e-01,  1.33765782e+00, ...,\n",
       "         6.52673807e-01,  3.19879228e-01,  6.79000000e+01],\n",
       "       [ 1.22517000e+05,  1.55478327e+00, -1.05940877e+00, ...,\n",
       "        -3.55714131e-02, -8.57818648e-03,  2.29880000e+02],\n",
       "       [ 8.13240000e+04, -2.37304316e+00,  2.56343777e+00, ...,\n",
       "        -9.07652241e-01, -1.48522812e-01,  2.41800000e+01]]), task=Task(task_type=<TaskTypesEnum.classification: ('classification',)>, task_params=None), data_type=<DataTypesEnum.table: 'feature_table'>, predict=array([1.  , 0.73, 0.27, 1.  , 1.  , 0.  , 1.  , 0.06, 1.  , 0.  , 0.08,\n",
       "       0.74, 0.  , 0.  , 1.  , 1.  , 0.26, 1.  , 0.59, 0.  , 0.  , 0.  ,\n",
       "       1.  , 1.  , 0.  , 0.  , 0.  , 1.  , 0.  , 1.  , 1.  , 0.  , 0.  ,\n",
       "       0.01, 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.17, 1.  , 1.  , 0.  ,\n",
       "       0.05, 1.  , 0.08, 1.  , 1.  , 1.  , 1.  , 0.  , 0.99, 0.  , 0.24,\n",
       "       0.  , 1.  , 1.  , 1.  , 0.  , 1.  , 0.25, 0.  , 0.  , 1.  , 0.73,\n",
       "       0.05, 1.  , 0.  , 0.54, 0.01, 1.  , 0.03, 0.  , 0.  , 1.  , 1.  ,\n",
       "       1.  , 0.  , 1.  , 1.  , 1.  , 0.  , 1.  , 0.07, 0.  , 1.  , 0.  ,\n",
       "       0.  , 0.  , 1.  , 0.  , 1.  , 0.  , 1.  , 1.  , 0.  , 1.  , 0.99,\n",
       "       0.  , 1.  , 1.  , 1.  , 0.02, 1.  , 0.  , 1.  , 1.  , 0.  , 0.  ,\n",
       "       0.  , 0.24, 1.  , 1.  , 0.07, 1.  , 1.  , 0.  , 1.  , 0.1 , 1.  ,\n",
       "       0.  , 0.01, 0.  , 0.98, 1.  , 0.02, 1.  , 0.95, 0.  , 0.05, 0.  ,\n",
       "       1.  , 1.  , 0.99, 0.03, 1.  , 0.06, 0.03, 1.  , 0.  , 0.03, 0.  ,\n",
       "       0.  , 1.  , 0.05, 0.  , 0.  , 1.  , 0.11, 0.93, 0.  , 0.01, 1.  ,\n",
       "       0.  , 1.  , 1.  , 0.11, 1.  , 0.  , 0.  , 0.  , 1.  , 1.  , 0.  ,\n",
       "       1.  , 1.  , 0.  , 0.  , 1.  , 0.13, 0.  , 0.16, 0.  , 0.02, 0.91,\n",
       "       1.  , 0.  , 0.  , 1.  , 0.03, 0.71, 0.  , 0.  , 0.  , 1.  , 0.98,\n",
       "       0.  , 0.06, 1.  , 1.  , 1.  , 1.  , 0.  , 0.  , 1.  , 1.  , 1.  ,\n",
       "       1.  , 0.  , 1.  , 1.  , 0.03, 1.  , 0.  , 0.  , 0.96, 0.  , 1.  ,\n",
       "       1.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.13, 0.  , 0.  , 1.  , 0.2 ,\n",
       "       0.  , 0.07, 0.  , 1.  , 0.79, 0.  , 1.  , 1.  , 1.  , 1.  , 1.  ,\n",
       "       1.  , 0.  , 1.  , 1.  , 1.  , 1.  , 1.  , 0.02, 0.05, 1.  , 0.76,\n",
       "       1.  , 0.16, 0.  , 0.  , 0.  , 1.  , 1.  , 0.  , 0.18, 0.  , 0.04,\n",
       "       1.  , 0.  , 0.  , 1.  , 0.19, 0.  , 0.99, 1.  , 1.  , 1.  , 1.  ,\n",
       "       1.  , 0.04, 0.  , 0.01, 1.  , 0.  , 0.  , 0.  , 0.02, 0.  , 1.  ,\n",
       "       1.  , 1.  , 0.03, 0.  , 0.  , 0.  , 0.02, 1.  , 1.  , 1.  , 0.  ,\n",
       "       0.  , 0.99, 0.87, 1.  , 1.  , 0.06, 0.99, 0.12, 0.9 , 0.  , 1.  ,\n",
       "       0.  , 0.15, 1.  , 1.  , 0.  , 1.  , 1.  , 0.86, 1.  , 0.33, 0.  ,\n",
       "       0.02, 0.  , 1.  , 0.  , 1.  , 0.  , 0.  , 0.03, 0.  , 1.  , 0.91,\n",
       "       0.  , 0.04, 1.  , 1.  , 0.  , 1.  , 0.  , 1.  , 0.  , 1.  , 0.  ,\n",
       "       0.82, 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.01, 0.04, 0.28,\n",
       "       1.  , 1.  , 1.  , 1.  , 0.  , 0.  , 0.  , 1.  , 0.01, 1.  , 1.  ,\n",
       "       0.  , 0.  , 0.  , 0.  , 0.  , 0.81, 1.  , 1.  , 0.  , 0.01, 1.  ,\n",
       "       0.  , 0.  , 1.  , 0.  , 0.  , 1.  , 0.  , 0.  , 1.  , 0.  , 0.  ,\n",
       "       0.  , 1.  , 0.03, 0.01, 1.  , 0.97, 1.  , 0.06, 1.  , 1.  , 0.01,\n",
       "       0.  , 0.09, 1.  , 1.  , 1.  , 0.55, 0.  , 0.  , 1.  , 1.  , 0.79,\n",
       "       0.  , 0.  , 0.  , 0.11, 0.92, 1.  , 0.  , 0.04, 1.  , 0.  , 0.04,\n",
       "       1.  , 0.71, 1.  , 1.  , 0.79, 1.  , 0.95, 0.  , 1.  , 0.  , 1.  ,\n",
       "       0.  , 0.  , 1.  , 0.01, 1.  , 1.  , 1.  , 1.  , 0.  , 0.  , 0.68,\n",
       "       1.  , 0.  , 0.01, 0.65, 0.  , 1.  , 0.  , 0.  , 0.  , 1.  , 0.  ,\n",
       "       0.  , 1.  , 1.  , 1.  , 0.67, 0.05, 1.  , 1.  , 1.  , 1.  , 1.  ,\n",
       "       1.  , 1.  , 0.03, 1.  , 0.  , 1.  , 1.  , 1.  , 0.01, 0.  , 1.  ,\n",
       "       0.01, 0.  , 0.  , 1.  , 1.  , 1.  , 0.  , 0.  , 0.  , 0.06, 1.  ,\n",
       "       0.  , 1.  , 1.  , 1.  , 0.  , 1.  , 0.  , 0.17, 0.99, 0.  , 0.04,\n",
       "       1.  , 1.  , 1.  , 0.72, 1.  , 0.13, 1.  , 0.  , 0.  , 0.02, 1.  ,\n",
       "       1.  , 1.  , 1.  , 1.  , 0.  , 0.16, 0.  , 0.55, 1.  , 0.81, 1.  ,\n",
       "       0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 1.  , 0.  , 1.  , 0.01, 1.  ,\n",
       "       0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.93,\n",
       "       1.  , 1.  , 0.  , 1.  , 0.  , 1.  , 1.  , 1.  , 0.02, 0.64, 1.  ,\n",
       "       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 0.03, 1.  , 1.  , 0.03, 1.  ,\n",
       "       0.  , 1.  , 0.  , 1.  , 0.  , 1.  , 1.  , 1.  , 0.  , 1.  , 1.  ,\n",
       "       0.08, 1.  , 1.  , 0.  , 0.78, 0.  , 0.62, 0.62, 0.  , 0.  , 1.  ,\n",
       "       1.  , 1.  , 0.  , 0.22, 0.21, 0.31, 1.  , 1.  , 0.  , 0.01, 1.  ,\n",
       "       0.98, 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  ,\n",
       "       0.  , 0.  , 1.  , 0.  , 1.  , 0.03, 1.  , 0.  , 0.99, 1.  , 1.  ,\n",
       "       1.  , 1.  , 1.  , 0.  , 1.  , 0.  , 1.  , 1.  , 1.  , 0.01, 1.  ,\n",
       "       0.94, 0.  , 0.  , 1.  , 0.  , 1.  , 1.  , 0.34, 0.16, 1.  , 1.  ,\n",
       "       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 0.01, 1.  , 1.  , 1.  , 0.04,\n",
       "       1.  , 0.1 , 0.  , 1.  , 0.  , 0.  , 0.  , 1.  , 0.2 , 0.11, 1.  ,\n",
       "       0.06, 0.  , 0.  , 0.96, 0.19, 1.  , 1.  , 0.  , 0.  , 0.  , 0.8 ,\n",
       "       0.21, 1.  , 1.  , 1.  , 1.  , 0.  , 1.  , 0.  , 0.  , 0.  , 1.  ,\n",
       "       0.  , 0.  , 1.  , 0.  , 1.  , 0.  , 0.02, 1.  , 1.  , 1.  , 0.  ,\n",
       "       1.  , 0.  , 1.  , 0.03, 0.  , 0.99, 1.  , 1.  , 0.  , 0.  , 0.  ,\n",
       "       1.  , 0.79, 0.  , 1.  , 0.3 , 1.  , 1.  , 0.  , 1.  , 0.  , 0.05,\n",
       "       1.  , 1.  , 1.  , 0.03, 0.02, 0.01, 1.  , 1.  , 0.  , 0.01, 0.02,\n",
       "       0.  , 0.  , 1.  , 1.  , 0.  , 0.97, 0.01, 0.  , 0.  , 1.  , 0.  ,\n",
       "       0.  , 0.05, 1.  , 0.74, 1.  , 1.  , 0.  , 1.  , 1.  , 1.  , 0.  ,\n",
       "       1.  , 1.  , 1.  , 0.85, 0.02, 1.  , 1.  , 0.  , 1.  , 1.  , 0.01,\n",
       "       0.08, 1.  , 1.  , 0.74, 0.  , 0.  , 0.03, 0.  , 0.03, 0.01, 0.  ,\n",
       "       0.01, 1.  , 0.  , 0.05, 0.  , 1.  , 0.  , 0.75, 0.  , 1.  , 1.  ,\n",
       "       0.  , 0.14, 0.15, 0.  , 0.  , 0.1 , 0.03, 0.  , 1.  , 0.  , 1.  ,\n",
       "       0.  , 0.  , 0.  , 1.  , 0.  , 0.  ]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.fit(train_data, use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_tuning_predicted = chain.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9854838022\n"
     ]
    }
   ],
   "source": [
    "bfr_tun_roc_auc = roc_auc(y_true=test_data.target,\n",
    "                          y_score=before_tuning_predicted.predict)\n",
    "print(round(bfr_tun_roc_auc, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.03659277254782873, 0.9796747967479674, 0.9554083993722065)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = precision_score(y_true=test_data.target,y_pred=before_tuning_predicted.predict.round())\n",
    "r = recall_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round())\n",
    "a = accuracy_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round())\n",
    "p, r, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03659277254782873"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "482/(482+12690)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2 , 0.09, 0.23, ..., 0.  , 0.  , 0.17])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_tuning_predicted.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[271625,  12690],\n",
       "       [    10,    482]], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_data.target, before_tuning_predicted.predict.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9796747967479674"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "482/(482+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.96      0.98    284315\n",
      "         1.0       0.04      0.98      0.07       492\n",
      "\n",
      "    accuracy                           0.96    284807\n",
      "   macro avg       0.52      0.97      0.52    284807\n",
      "weighted avg       1.00      0.96      0.98    284807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_true=test_data.target,y_pred=before_tuning_predicted.predict.round())\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9856229113792114 0.03766507775259827 0.9796747967479674 0.9567250804931059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9856229113792114"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_chain_from_automl(train_file_path: str, test_file_path: str,\n",
    "                          max_run_time: timedelta = timedelta(minutes=10)):\n",
    "    train_data = InputData.from_csv(train_file_path)\n",
    "    test_data = InputData.from_csv(test_file_path)\n",
    "    \n",
    "    \n",
    "    testing_target = test_data.target\n",
    "\n",
    "    #1 model\n",
    "    chain = Chain()\n",
    "    node_logit = PrimaryNode('logit')\n",
    "    node_logit.model.external_params = {'max_run_time_sec': max_run_time.seconds}\n",
    "    \n",
    "    node_lda = PrimaryNode('lda')\n",
    "    node_rf = SecondaryNode('rf')\n",
    "\n",
    "    node_rf.nodes_from = [node_logit, node_lda]\n",
    "\n",
    "    chain.add_node(node_rf)\n",
    "\n",
    "    chain.fit(train_data)\n",
    "    results = chain.predict(test_data)\n",
    "\n",
    "    roc_auc_value = roc_auc(y_true=testing_target,\n",
    "                            y_score=results.predict)\n",
    "    \n",
    "    \n",
    "    p = precision_score(y_true=testing_target,y_pred=results.predict.round())\n",
    "    r = recall_score(y_true=testing_target, y_pred=results.predict.round())\n",
    "    a = accuracy_score(y_true=testing_target, y_pred=results.predict.round())\n",
    "    \n",
    "    print(roc_auc_value, p, r, a)\n",
    "\n",
    "    return roc_auc_value\n",
    "\n",
    "run_chain_from_automl(train_file_path, test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_first = r'./export_dataframe.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path, test_file_path = create_multi_clf_examples_from_excel(file_path_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Тимур\\\\vir\\\\Scripts\\\\Fedot\\\\examples\\\\data\\\\export_dataframe\\\\train.csv',\n",
       " 'C:\\\\Users\\\\Тимур\\\\vir\\\\Scripts\\\\Fedot\\\\examples\\\\data\\\\export_dataframe\\\\test.csv')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_path, test_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = InputData.from_csv(train_file_path)\n",
    "test_data = InputData.from_csv(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_chain():\n",
    "#     first = PrimaryNode(model_type='logit')\n",
    "#     second = PrimaryNode(model_type='lda')\n",
    "    final = PrimaryNode(model_type='rf') #,\n",
    "                          #nodes_from=[first, second])\n",
    "\n",
    "    chain = Chain(final)\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = get_simple_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OutputData(idx=array([962., 762., 334., 889., 529., 468., 978., 231., 568.,  33.,  31.,\n",
       "       736., 192., 318., 495., 933., 309., 934., 715., 109., 411.,  77.,\n",
       "       680., 923.,  82., 404., 331., 605., 382., 539., 845.,   2., 101.,\n",
       "       371., 462., 453., 208., 884.,   5.,  54., 307., 857., 811., 447.,\n",
       "        97., 597., 204., 514., 628., 944., 527., 344., 854.,  25.,  84.,\n",
       "        10., 892., 770., 621., 365., 752., 118., 350., 250., 705., 940.,\n",
       "       196., 596.,  81., 958., 457., 611., 239., 277., 211., 708., 858.,\n",
       "       713., 227., 842., 497., 785., 482., 636., 346., 450., 824., 352.,\n",
       "         7., 155., 534., 422., 704., 380., 767., 559., 319., 591., 919.,\n",
       "       398., 523., 809., 501., 430., 982., 228., 764., 876., 212.,  79.,\n",
       "       148., 302., 545., 536., 327., 973., 799., 133., 543., 311., 599.,\n",
       "       423.,   0., 316., 697., 797., 328., 525., 970., 433., 172., 125.,\n",
       "       541., 879., 888.,  90., 924., 181., 274., 880.,  69., 291., 131.,\n",
       "       300., 802., 326., 144., 405., 787., 135., 532., 164.,  28., 783.,\n",
       "       193., 901., 629., 169., 728., 140., 173.,   6., 633., 638.,  73.,\n",
       "       849., 861., 238., 145., 781., 234., 220., 456., 481., 132., 974.,\n",
       "       528., 185.,  41., 602., 108., 581.,  56., 388., 424., 788., 846.,\n",
       "        24., 428., 790., 687., 685., 911., 338.,  51., 696., 931., 905.,\n",
       "       662., 264., 530., 673., 483., 507., 429.,  18., 706., 464., 903.,\n",
       "       722., 367.,  83.,  61., 439., 272., 285., 360., 354., 720., 278.,\n",
       "        12., 182., 368., 518., 753., 223., 851., 810., 803., 499., 575.,\n",
       "       601., 176., 657., 784., 578., 513., 786., 163., 248., 626., 902.,\n",
       "       963., 375., 412.,  74., 113., 631., 863., 390., 104., 114., 417.,\n",
       "       910., 409.,  92., 557.,  89., 336., 972., 906., 918., 598., 756.,\n",
       "       603.,  94.,  11., 396., 526.,  43.,  42., 329., 167., 478., 886.,\n",
       "       590., 745., 100., 426., 178., 444., 416., 692., 868., 650., 177.,\n",
       "       395., 667., 939., 945., 675., 383., 941., 257., 531., 335., 606.,\n",
       "        15.,   3., 867., 908., 256., 538., 948., 737., 572., 393., 222.,\n",
       "       179., 289., 967., 324., 583.,   9., 249.,  22., 356., 877., 847.,\n",
       "       340., 431., 544., 820., 203., 622.,  93., 551.,  68., 975., 284.,\n",
       "       872., 434., 153.,  75., 721., 446., 188., 271., 236., 487., 117.,\n",
       "       957., 512., 620., 584., 126., 116., 473., 684.,  57., 665., 914.,\n",
       "       369., 268.,  46., 349., 195., 983., 895., 881., 263., 443., 666.,\n",
       "       304., 341., 951., 149., 124., 907.,  50., 353., 912., 142., 470.,\n",
       "       399., 617., 320.,  19., 796., 981., 795., 407., 537., 740.,  38.,\n",
       "       175., 245., 885., 893., 743., 844., 154., 287., 595., 569., 732.,\n",
       "        17., 127., 322., 255., 649., 949., 190., 115., 625., 180., 301.,\n",
       "       800., 703., 714., 760., 653., 517., 968.,  45., 894., 157., 932.,\n",
       "       171.,  16., 511.,  48., 955., 827., 515., 677., 480., 283., 723.,\n",
       "       927., 225.,  26., 896., 437., 936., 364., 229.,  37., 950., 374.,\n",
       "       469., 952., 837., 695., 716., 194., 925., 850., 503., 954., 817.,\n",
       "       579., 953., 162., 693., 152., 672., 750., 822., 111., 226., 679.,\n",
       "       103., 421., 419., 739., 586., 961., 119.,  53., 151., 403., 930.,\n",
       "       207., 658., 830., 751.,   8., 816.,  36., 452., 651., 253., 303.,\n",
       "       735., 571., 623., 977., 710., 262., 610., 297., 414., 150., 777.,\n",
       "       640., 874., 550., 780., 488., 147., 146., 711., 916., 891., 659.,\n",
       "       348., 463., 325., 186., 123., 839., 608., 143., 943., 197., 609.,\n",
       "       279., 293., 400., 122., 183., 202., 438., 246., 415., 765., 754.,\n",
       "       757., 873., 129., 637., 402., 773., 759., 898., 219., 641., 900.,\n",
       "       741., 793., 904., 624., 825., 749., 386., 956., 509., 267., 806.,\n",
       "       441., 496., 112., 691., 232., 855., 607., 671., 373., 965., 829.,\n",
       "       233., 774., 676., 317., 648., 410., 883., 709., 358., 258., 744.,\n",
       "       627., 632., 282., 376., 384., 224., 938., 801., 472., 347., 505.,\n",
       "       639., 971., 913., 890., 619., 841., 645., 833., 556., 942., 577.,\n",
       "        85., 242., 698., 159., 524.,  35., 540., 170., 654., 935., 843.,\n",
       "       834., 929., 733.,  95., 563., 240., 742., 574., 690., 460., 553.,\n",
       "       864., 206., 392., 794., 397., 766., 835., 217.,   4., 768., 642.,\n",
       "       882., 612., 738., 546., 725., 683.,  98., 804., 727., 573., 406.,\n",
       "       502.,  47.,  32., 779., 200., 134.,  27., 866., 230., 489., 772.,\n",
       "       378., 288., 418., 674., 391., 592., 498., 138.,  62., 471., 647.,\n",
       "       128., 960., 520., 838., 947.,  64., 812.,  14., 156.,  40., 492.,\n",
       "       379., 187., 763., 216., 791.,  52., 337., 748., 719., 724., 295.,\n",
       "       701., 251., 726., 461., 455., 980., 815., 862., 269., 201., 161.,\n",
       "       555., 729., 401., 702., 476., 821., 771., 105., 565., 389.,   1.,\n",
       "       922., 966., 561.,  80., 205.,  34., 775., 508., 427., 454., 366.,\n",
       "        91., 339., 897., 564., 345., 776., 241.,  13., 315., 600., 387.,\n",
       "       273., 166., 840., 976., 646., 818., 484., 964., 504., 831., 243.,\n",
       "       566., 875., 562., 686., 189., 782., 699., 475., 681., 510.,  58.,\n",
       "       474., 560., 856., 747., 252.,  21., 313., 459., 160., 276., 191.,\n",
       "       385., 805., 413., 491., 343., 769., 308., 661., 130., 663., 871.,\n",
       "        99., 372.,  87., 458., 330., 214., 466., 121., 614.,  20., 700.,\n",
       "        71., 106., 270., 860., 435., 102.]), features=array([[ 1.58638000e+05, -5.97611932e+00, -7.19697963e+00, ...,\n",
       "         5.65846302e-01, -1.03410719e+00,  2.96000000e+02],\n",
       "       [ 8.47890000e+04, -1.43086373e+00, -8.02528699e-01, ...,\n",
       "        -1.20351073e-01,  3.55941734e-02,  3.54330000e+02],\n",
       "       [ 1.45056000e+05, -2.71359479e-01,  1.29843749e+00, ...,\n",
       "         1.33778090e-01,  1.70118018e-01,  4.91600000e+01],\n",
       "       ...,\n",
       "       [ 1.28471000e+05,  9.09123839e-01,  1.33765782e+00, ...,\n",
       "         6.52673807e-01,  3.19879228e-01,  6.79000000e+01],\n",
       "       [ 1.22517000e+05,  1.55478327e+00, -1.05940877e+00, ...,\n",
       "        -3.55714131e-02, -8.57818648e-03,  2.29880000e+02],\n",
       "       [ 8.13240000e+04, -2.37304316e+00,  2.56343777e+00, ...,\n",
       "        -9.07652241e-01, -1.48522812e-01,  2.41800000e+01]]), task=Task(task_type=<TaskTypesEnum.classification: ('classification',)>, task_params=None), data_type=<DataTypesEnum.table: 'feature_table'>, predict=array([0.98, 0.71, 0.14, 1.  , 1.  , 0.03, 1.  , 0.03, 1.  , 0.  , 0.05,\n",
       "       0.62, 0.08, 0.02, 1.  , 1.  , 0.1 , 1.  , 0.69, 0.  , 0.04, 0.  ,\n",
       "       1.  , 0.95, 0.05, 0.01, 0.09, 1.  , 0.01, 1.  , 1.  , 0.05, 0.02,\n",
       "       0.  , 0.  , 0.02, 0.02, 0.98, 0.02, 0.05, 0.15, 0.96, 1.  , 0.  ,\n",
       "       0.04, 0.99, 0.02, 1.  , 0.98, 0.94, 1.  , 0.  , 0.99, 0.03, 0.11,\n",
       "       0.02, 1.  , 1.  , 1.  , 0.11, 0.94, 0.27, 0.07, 0.04, 1.  , 0.89,\n",
       "       0.06, 1.  , 0.04, 0.73, 0.02, 1.  , 0.02, 0.  , 0.  , 0.98, 1.  ,\n",
       "       1.  , 0.  , 1.  , 1.  , 1.  , 0.  , 1.  , 0.04, 0.01, 1.  , 0.14,\n",
       "       0.  , 0.02, 1.  , 0.  , 1.  , 0.  , 1.  , 1.  , 0.  , 1.  , 1.  ,\n",
       "       0.02, 1.  , 1.  , 1.  , 0.1 , 0.98, 0.  , 1.  , 0.97, 0.01, 0.  ,\n",
       "       0.  , 0.14, 1.  , 1.  , 0.13, 0.89, 1.  , 0.  , 1.  , 0.08, 1.  ,\n",
       "       0.  , 0.07, 0.  , 0.93, 1.  , 0.01, 1.  , 1.  , 0.04, 0.07, 0.03,\n",
       "       1.  , 0.99, 0.74, 0.04, 1.  , 0.06, 0.16, 0.99, 0.02, 0.01, 0.03,\n",
       "       0.  , 1.  , 0.11, 0.  , 0.09, 0.99, 0.18, 0.87, 0.  , 0.07, 1.  ,\n",
       "       0.  , 1.  , 1.  , 0.14, 1.  , 0.  , 0.01, 0.02, 0.99, 1.  , 0.01,\n",
       "       1.  , 0.99, 0.  , 0.  , 0.99, 0.08, 0.02, 0.09, 0.04, 0.01, 0.83,\n",
       "       1.  , 0.  , 0.02, 1.  , 0.02, 0.74, 0.01, 0.02, 0.  , 1.  , 1.  ,\n",
       "       0.03, 0.01, 1.  , 1.  , 1.  , 1.  , 0.03, 0.01, 0.99, 1.  , 1.  ,\n",
       "       0.9 , 0.  , 0.98, 1.  , 0.19, 1.  , 0.02, 0.03, 0.95, 0.02, 1.  ,\n",
       "       1.  , 0.09, 0.  , 0.  , 0.02, 0.05, 0.03, 0.03, 0.  , 1.  , 0.02,\n",
       "       0.02, 0.08, 0.01, 1.  , 0.83, 0.02, 0.98, 1.  , 1.  , 1.  , 1.  ,\n",
       "       1.  , 0.01, 1.  , 1.  , 0.99, 1.  , 1.  , 0.06, 0.01, 1.  , 0.68,\n",
       "       0.99, 0.01, 0.  , 0.11, 0.  , 1.  , 1.  , 0.04, 0.1 , 0.02, 0.  ,\n",
       "       0.98, 0.  , 0.01, 1.  , 0.08, 0.03, 1.  , 1.  , 1.  , 1.  , 1.  ,\n",
       "       1.  , 0.15, 0.01, 0.01, 1.  , 0.17, 0.06, 0.  , 0.16, 0.1 , 1.  ,\n",
       "       1.  , 1.  , 0.08, 0.05, 0.07, 0.06, 0.2 , 1.  , 0.99, 1.  , 0.01,\n",
       "       0.  , 1.  , 0.77, 0.99, 0.99, 0.18, 0.96, 0.08, 0.91, 0.08, 1.  ,\n",
       "       0.07, 0.01, 0.97, 1.  , 0.  , 0.99, 1.  , 0.72, 1.  , 0.25, 0.04,\n",
       "       0.1 , 0.03, 1.  , 0.02, 1.  , 0.02, 0.01, 0.01, 0.  , 0.99, 0.76,\n",
       "       0.05, 0.18, 1.  , 1.  , 0.05, 1.  , 0.02, 1.  , 0.07, 1.  , 0.  ,\n",
       "       0.85, 0.01, 0.  , 0.02, 1.  , 0.  , 0.02, 0.  , 0.08, 0.  , 0.24,\n",
       "       1.  , 1.  , 1.  , 1.  , 0.03, 0.03, 0.04, 0.98, 0.01, 1.  , 0.93,\n",
       "       0.01, 0.02, 0.05, 0.  , 0.02, 0.82, 1.  , 0.99, 0.01, 0.  , 1.  ,\n",
       "       0.05, 0.02, 1.  , 0.02, 0.  , 1.  , 0.06, 0.  , 1.  , 0.  , 0.01,\n",
       "       0.12, 1.  , 0.11, 0.  , 1.  , 0.97, 1.  , 0.02, 1.  , 1.  , 0.04,\n",
       "       0.03, 0.  , 1.  , 0.96, 1.  , 0.71, 0.01, 0.1 , 0.87, 1.  , 0.66,\n",
       "       0.01, 0.02, 0.  , 0.01, 0.75, 1.  , 0.02, 0.03, 1.  , 0.03, 0.01,\n",
       "       1.  , 0.68, 1.  , 1.  , 0.67, 1.  , 0.73, 0.  , 1.  , 0.02, 1.  ,\n",
       "       0.05, 0.01, 1.  , 0.03, 1.  , 1.  , 1.  , 1.  , 0.06, 0.01, 0.71,\n",
       "       0.98, 0.01, 0.  , 0.63, 0.  , 1.  , 0.04, 0.03, 0.14, 1.  , 0.03,\n",
       "       0.01, 1.  , 1.  , 0.98, 0.7 , 0.  , 1.  , 0.99, 1.  , 0.99, 0.99,\n",
       "       1.  , 0.96, 0.  , 0.93, 0.06, 1.  , 0.96, 1.  , 0.03, 0.03, 1.  ,\n",
       "       0.  , 0.01, 0.01, 1.  , 1.  , 1.  , 0.07, 0.11, 0.01, 0.01, 0.99,\n",
       "       0.13, 1.  , 1.  , 1.  , 0.03, 1.  , 0.  , 0.08, 1.  , 0.07, 0.  ,\n",
       "       0.97, 1.  , 1.  , 0.75, 0.97, 0.04, 1.  , 0.03, 0.  , 0.05, 0.87,\n",
       "       1.  , 0.86, 1.  , 0.96, 0.  , 0.05, 0.02, 0.75, 0.99, 0.72, 1.  ,\n",
       "       0.04, 0.03, 0.06, 0.01, 0.  , 1.  , 1.  , 0.01, 0.99, 0.14, 1.  ,\n",
       "       0.08, 0.  , 0.  , 0.08, 0.  , 0.  , 0.  , 0.03, 0.04, 1.  , 1.  ,\n",
       "       1.  , 0.88, 0.13, 1.  , 0.06, 0.86, 1.  , 0.95, 0.  , 0.65, 1.  ,\n",
       "       1.  , 1.  , 1.  , 0.99, 0.98, 0.99, 0.01, 1.  , 1.  , 0.09, 1.  ,\n",
       "       0.  , 0.99, 0.  , 1.  , 0.03, 1.  , 1.  , 1.  , 0.  , 1.  , 0.99,\n",
       "       0.18, 0.83, 1.  , 0.02, 0.78, 0.09, 0.76, 0.72, 0.01, 0.  , 1.  ,\n",
       "       1.  , 1.  , 0.01, 0.2 , 0.1 , 0.22, 1.  , 1.  , 0.  , 0.08, 1.  ,\n",
       "       0.97, 1.  , 0.87, 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 0.78,\n",
       "       0.09, 0.01, 1.  , 0.  , 0.96, 0.13, 1.  , 0.  , 1.  , 1.  , 1.  ,\n",
       "       0.97, 0.98, 1.  , 0.02, 1.  , 0.09, 1.  , 1.  , 0.99, 0.  , 1.  ,\n",
       "       0.74, 0.  , 0.02, 1.  , 0.16, 1.  , 1.  , 0.13, 0.17, 1.  , 1.  ,\n",
       "       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 0.02, 1.  , 0.99, 1.  , 0.02,\n",
       "       1.  , 0.02, 0.02, 1.  , 0.  , 0.12, 0.02, 1.  , 0.14, 0.08, 0.83,\n",
       "       0.04, 0.03, 0.05, 0.79, 0.01, 1.  , 1.  , 0.  , 0.01, 0.  , 0.64,\n",
       "       0.04, 1.  , 1.  , 1.  , 1.  , 0.01, 1.  , 0.02, 0.  , 0.02, 1.  ,\n",
       "       0.02, 0.  , 0.99, 0.02, 1.  , 0.  , 0.01, 1.  , 1.  , 1.  , 0.02,\n",
       "       1.  , 0.01, 1.  , 0.  , 0.08, 0.99, 1.  , 0.99, 0.05, 0.05, 0.01,\n",
       "       1.  , 0.81, 0.  , 0.99, 0.16, 1.  , 1.  , 0.07, 1.  , 0.  , 0.  ,\n",
       "       0.99, 1.  , 1.  , 0.04, 0.21, 0.  , 1.  , 1.  , 0.01, 0.01, 0.05,\n",
       "       0.12, 0.01, 0.95, 1.  , 0.  , 0.97, 0.01, 0.02, 0.06, 1.  , 0.01,\n",
       "       0.01, 0.  , 1.  , 0.76, 1.  , 0.99, 0.  , 0.98, 1.  , 1.  , 0.03,\n",
       "       1.  , 0.97, 1.  , 0.77, 0.13, 0.98, 1.  , 0.  , 1.  , 1.  , 0.02,\n",
       "       0.  , 1.  , 0.97, 0.63, 0.  , 0.02, 0.02, 0.02, 0.06, 0.02, 0.05,\n",
       "       0.  , 1.  , 0.07, 0.09, 0.  , 1.  , 0.02, 0.75, 0.02, 1.  , 1.  ,\n",
       "       0.02, 0.23, 0.13, 0.  , 0.04, 0.09, 0.03, 0.03, 1.  , 0.05, 0.99,\n",
       "       0.01, 0.05, 0.  , 1.  , 0.01, 0.04]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.fit(train_data, use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_tuning_predicted = chain.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9788188002\n"
     ]
    }
   ],
   "source": [
    "bfr_tun_roc_auc = roc_auc(y_true=test_data.target,\n",
    "                          y_score=before_tuning_predicted.predict)\n",
    "print(round(bfr_tun_roc_auc, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9560439560439561, 0.8877551020408163, 0.9238578680203046)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = precision_score(test_data.target,before_tuning_predicted.predict.round())\n",
    "r = recall_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round())\n",
    "a = accuracy_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round())\n",
    "p, r, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logit',\n",
       " 'lda',\n",
       " 'qda',\n",
       " 'dt',\n",
       " 'rf',\n",
       " 'mlp',\n",
       " 'knn',\n",
       " 'svc',\n",
       " 'xgboost',\n",
       " 'bernb',\n",
       " 'direct_data_model']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_model_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logit'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_model_types[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_chain(model_type):\n",
    "    \n",
    "    final = PrimaryNode(model_type='{}'.format(model_type))                    \n",
    "    chain = Chain(final)\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = get_simple_chain('rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OutputData(idx=array([962., 762., 334., 889., 529., 468., 978., 231., 568.,  33.,  31.,\n",
       "       736., 192., 318., 495., 933., 309., 934., 715., 109., 411.,  77.,\n",
       "       680., 923.,  82., 404., 331., 605., 382., 539., 845.,   2., 101.,\n",
       "       371., 462., 453., 208., 884.,   5.,  54., 307., 857., 811., 447.,\n",
       "        97., 597., 204., 514., 628., 944., 527., 344., 854.,  25.,  84.,\n",
       "        10., 892., 770., 621., 365., 752., 118., 350., 250., 705., 940.,\n",
       "       196., 596.,  81., 958., 457., 611., 239., 277., 211., 708., 858.,\n",
       "       713., 227., 842., 497., 785., 482., 636., 346., 450., 824., 352.,\n",
       "         7., 155., 534., 422., 704., 380., 767., 559., 319., 591., 919.,\n",
       "       398., 523., 809., 501., 430., 982., 228., 764., 876., 212.,  79.,\n",
       "       148., 302., 545., 536., 327., 973., 799., 133., 543., 311., 599.,\n",
       "       423.,   0., 316., 697., 797., 328., 525., 970., 433., 172., 125.,\n",
       "       541., 879., 888.,  90., 924., 181., 274., 880.,  69., 291., 131.,\n",
       "       300., 802., 326., 144., 405., 787., 135., 532., 164.,  28., 783.,\n",
       "       193., 901., 629., 169., 728., 140., 173.,   6., 633., 638.,  73.,\n",
       "       849., 861., 238., 145., 781., 234., 220., 456., 481., 132., 974.,\n",
       "       528., 185.,  41., 602., 108., 581.,  56., 388., 424., 788., 846.,\n",
       "        24., 428., 790., 687., 685., 911., 338.,  51., 696., 931., 905.,\n",
       "       662., 264., 530., 673., 483., 507., 429.,  18., 706., 464., 903.,\n",
       "       722., 367.,  83.,  61., 439., 272., 285., 360., 354., 720., 278.,\n",
       "        12., 182., 368., 518., 753., 223., 851., 810., 803., 499., 575.,\n",
       "       601., 176., 657., 784., 578., 513., 786., 163., 248., 626., 902.,\n",
       "       963., 375., 412.,  74., 113., 631., 863., 390., 104., 114., 417.,\n",
       "       910., 409.,  92., 557.,  89., 336., 972., 906., 918., 598., 756.,\n",
       "       603.,  94.,  11., 396., 526.,  43.,  42., 329., 167., 478., 886.,\n",
       "       590., 745., 100., 426., 178., 444., 416., 692., 868., 650., 177.,\n",
       "       395., 667., 939., 945., 675., 383., 941., 257., 531., 335., 606.,\n",
       "        15.,   3., 867., 908., 256., 538., 948., 737., 572., 393., 222.,\n",
       "       179., 289., 967., 324., 583.,   9., 249.,  22., 356., 877., 847.,\n",
       "       340., 431., 544., 820., 203., 622.,  93., 551.,  68., 975., 284.,\n",
       "       872., 434., 153.,  75., 721., 446., 188., 271., 236., 487., 117.,\n",
       "       957., 512., 620., 584., 126., 116., 473., 684.,  57., 665., 914.,\n",
       "       369., 268.,  46., 349., 195., 983., 895., 881., 263., 443., 666.,\n",
       "       304., 341., 951., 149., 124., 907.,  50., 353., 912., 142., 470.,\n",
       "       399., 617., 320.,  19., 796., 981., 795., 407., 537., 740.,  38.,\n",
       "       175., 245., 885., 893., 743., 844., 154., 287., 595., 569., 732.,\n",
       "        17., 127., 322., 255., 649., 949., 190., 115., 625., 180., 301.,\n",
       "       800., 703., 714., 760., 653., 517., 968.,  45., 894., 157., 932.,\n",
       "       171.,  16., 511.,  48., 955., 827., 515., 677., 480., 283., 723.,\n",
       "       927., 225.,  26., 896., 437., 936., 364., 229.,  37., 950., 374.,\n",
       "       469., 952., 837., 695., 716., 194., 925., 850., 503., 954., 817.,\n",
       "       579., 953., 162., 693., 152., 672., 750., 822., 111., 226., 679.,\n",
       "       103., 421., 419., 739., 586., 961., 119.,  53., 151., 403., 930.,\n",
       "       207., 658., 830., 751.,   8., 816.,  36., 452., 651., 253., 303.,\n",
       "       735., 571., 623., 977., 710., 262., 610., 297., 414., 150., 777.,\n",
       "       640., 874., 550., 780., 488., 147., 146., 711., 916., 891., 659.,\n",
       "       348., 463., 325., 186., 123., 839., 608., 143., 943., 197., 609.,\n",
       "       279., 293., 400., 122., 183., 202., 438., 246., 415., 765., 754.,\n",
       "       757., 873., 129., 637., 402., 773., 759., 898., 219., 641., 900.,\n",
       "       741., 793., 904., 624., 825., 749., 386., 956., 509., 267., 806.,\n",
       "       441., 496., 112., 691., 232., 855., 607., 671., 373., 965., 829.,\n",
       "       233., 774., 676., 317., 648., 410., 883., 709., 358., 258., 744.,\n",
       "       627., 632., 282., 376., 384., 224., 938., 801., 472., 347., 505.,\n",
       "       639., 971., 913., 890., 619., 841., 645., 833., 556., 942., 577.,\n",
       "        85., 242., 698., 159., 524.,  35., 540., 170., 654., 935., 843.,\n",
       "       834., 929., 733.,  95., 563., 240., 742., 574., 690., 460., 553.,\n",
       "       864., 206., 392., 794., 397., 766., 835., 217.,   4., 768., 642.,\n",
       "       882., 612., 738., 546., 725., 683.,  98., 804., 727., 573., 406.,\n",
       "       502.,  47.,  32., 779., 200., 134.,  27., 866., 230., 489., 772.,\n",
       "       378., 288., 418., 674., 391., 592., 498., 138.,  62., 471., 647.,\n",
       "       128., 960., 520., 838., 947.,  64., 812.,  14., 156.,  40., 492.,\n",
       "       379., 187., 763., 216., 791.,  52., 337., 748., 719., 724., 295.,\n",
       "       701., 251., 726., 461., 455., 980., 815., 862., 269., 201., 161.,\n",
       "       555., 729., 401., 702., 476., 821., 771., 105., 565., 389.,   1.,\n",
       "       922., 966., 561.,  80., 205.,  34., 775., 508., 427., 454., 366.,\n",
       "        91., 339., 897., 564., 345., 776., 241.,  13., 315., 600., 387.,\n",
       "       273., 166., 840., 976., 646., 818., 484., 964., 504., 831., 243.,\n",
       "       566., 875., 562., 686., 189., 782., 699., 475., 681., 510.,  58.,\n",
       "       474., 560., 856., 747., 252.,  21., 313., 459., 160., 276., 191.,\n",
       "       385., 805., 413., 491., 343., 769., 308., 661., 130., 663., 871.,\n",
       "        99., 372.,  87., 458., 330., 214., 466., 121., 614.,  20., 700.,\n",
       "        71., 106., 270., 860., 435., 102.]), features=array([[ 1.58638000e+05, -5.97611932e+00, -7.19697963e+00, ...,\n",
       "         5.65846302e-01, -1.03410719e+00,  2.96000000e+02],\n",
       "       [ 8.47890000e+04, -1.43086373e+00, -8.02528699e-01, ...,\n",
       "        -1.20351073e-01,  3.55941734e-02,  3.54330000e+02],\n",
       "       [ 1.45056000e+05, -2.71359479e-01,  1.29843749e+00, ...,\n",
       "         1.33778090e-01,  1.70118018e-01,  4.91600000e+01],\n",
       "       ...,\n",
       "       [ 1.28471000e+05,  9.09123839e-01,  1.33765782e+00, ...,\n",
       "         6.52673807e-01,  3.19879228e-01,  6.79000000e+01],\n",
       "       [ 1.22517000e+05,  1.55478327e+00, -1.05940877e+00, ...,\n",
       "        -3.55714131e-02, -8.57818648e-03,  2.29880000e+02],\n",
       "       [ 8.13240000e+04, -2.37304316e+00,  2.56343777e+00, ...,\n",
       "        -9.07652241e-01, -1.48522812e-01,  2.41800000e+01]]), task=Task(task_type=<TaskTypesEnum.classification: ('classification',)>, task_params=None), data_type=<DataTypesEnum.table: 'feature_table'>, predict=array([0.99, 0.67, 0.13, 1.  , 1.  , 0.02, 1.  , 0.08, 1.  , 0.  , 0.06,\n",
       "       0.66, 0.03, 0.  , 1.  , 1.  , 0.08, 1.  , 0.74, 0.  , 0.03, 0.03,\n",
       "       1.  , 0.99, 0.03, 0.01, 0.07, 1.  , 0.  , 1.  , 1.  , 0.06, 0.  ,\n",
       "       0.01, 0.  , 0.  , 0.  , 1.  , 0.  , 0.06, 0.19, 0.95, 1.  , 0.  ,\n",
       "       0.08, 0.99, 0.03, 1.  , 1.  , 0.97, 1.  , 0.02, 1.  , 0.05, 0.17,\n",
       "       0.02, 1.  , 1.  , 1.  , 0.08, 1.  , 0.27, 0.06, 0.02, 0.98, 0.85,\n",
       "       0.01, 0.96, 0.01, 0.71, 0.05, 1.  , 0.02, 0.01, 0.02, 0.99, 1.  ,\n",
       "       1.  , 0.  , 1.  , 0.99, 1.  , 0.  , 1.  , 0.05, 0.01, 0.99, 0.07,\n",
       "       0.  , 0.03, 1.  , 0.  , 1.  , 0.01, 1.  , 1.  , 0.  , 1.  , 1.  ,\n",
       "       0.  , 1.  , 1.  , 1.  , 0.15, 0.99, 0.  , 1.  , 1.  , 0.02, 0.01,\n",
       "       0.  , 0.12, 1.  , 1.  , 0.14, 0.83, 1.  , 0.02, 1.  , 0.08, 1.  ,\n",
       "       0.  , 0.08, 0.01, 0.98, 1.  , 0.02, 1.  , 1.  , 0.04, 0.07, 0.07,\n",
       "       1.  , 1.  , 0.78, 0.03, 1.  , 0.04, 0.08, 0.97, 0.02, 0.02, 0.03,\n",
       "       0.  , 1.  , 0.03, 0.  , 0.06, 1.  , 0.15, 0.81, 0.  , 0.07, 1.  ,\n",
       "       0.  , 0.99, 1.  , 0.15, 1.  , 0.  , 0.  , 0.03, 0.99, 1.  , 0.01,\n",
       "       1.  , 1.  , 0.  , 0.  , 0.98, 0.  , 0.  , 0.11, 0.01, 0.02, 0.71,\n",
       "       1.  , 0.01, 0.02, 1.  , 0.01, 0.72, 0.  , 0.  , 0.02, 1.  , 1.  ,\n",
       "       0.  , 0.02, 1.  , 1.  , 1.  , 1.  , 0.02, 0.02, 1.  , 1.  , 1.  ,\n",
       "       0.85, 0.  , 0.97, 1.  , 0.15, 1.  , 0.03, 0.03, 0.91, 0.02, 1.  ,\n",
       "       1.  , 0.07, 0.04, 0.  , 0.01, 0.01, 0.08, 0.02, 0.02, 1.  , 0.  ,\n",
       "       0.02, 0.1 , 0.  , 0.98, 0.84, 0.03, 0.99, 1.  , 1.  , 0.99, 1.  ,\n",
       "       1.  , 0.  , 1.  , 1.  , 1.  , 1.  , 1.  , 0.08, 0.  , 1.  , 0.74,\n",
       "       0.96, 0.01, 0.  , 0.12, 0.  , 1.  , 1.  , 0.04, 0.06, 0.  , 0.01,\n",
       "       0.99, 0.  , 0.  , 1.  , 0.07, 0.05, 0.98, 1.  , 0.96, 1.  , 1.  ,\n",
       "       1.  , 0.09, 0.02, 0.03, 1.  , 0.22, 0.02, 0.04, 0.1 , 0.06, 1.  ,\n",
       "       1.  , 0.99, 0.11, 0.06, 0.08, 0.06, 0.14, 1.  , 1.  , 1.  , 0.  ,\n",
       "       0.01, 0.99, 0.74, 0.98, 1.  , 0.17, 0.96, 0.12, 0.81, 0.07, 1.  ,\n",
       "       0.06, 0.02, 0.99, 1.  , 0.  , 1.  , 1.  , 0.81, 1.  , 0.32, 0.06,\n",
       "       0.09, 0.04, 1.  , 0.  , 1.  , 0.01, 0.  , 0.  , 0.  , 1.  , 0.88,\n",
       "       0.05, 0.21, 1.  , 1.  , 0.17, 1.  , 0.02, 1.  , 0.07, 0.99, 0.  ,\n",
       "       0.87, 0.01, 0.  , 0.  , 1.  , 0.  , 0.02, 0.  , 0.14, 0.  , 0.2 ,\n",
       "       1.  , 1.  , 1.  , 1.  , 0.02, 0.05, 0.07, 0.98, 0.01, 1.  , 0.95,\n",
       "       0.01, 0.02, 0.04, 0.  , 0.03, 0.77, 1.  , 0.99, 0.03, 0.03, 0.99,\n",
       "       0.08, 0.04, 1.  , 0.02, 0.  , 1.  , 0.05, 0.  , 1.  , 0.01, 0.  ,\n",
       "       0.1 , 1.  , 0.11, 0.  , 1.  , 0.98, 1.  , 0.02, 1.  , 1.  , 0.05,\n",
       "       0.03, 0.01, 1.  , 1.  , 0.99, 0.73, 0.01, 0.08, 0.87, 1.  , 0.59,\n",
       "       0.02, 0.05, 0.03, 0.03, 0.68, 1.  , 0.  , 0.02, 0.99, 0.09, 0.01,\n",
       "       1.  , 0.61, 1.  , 1.  , 0.71, 1.  , 0.78, 0.03, 1.  , 0.01, 1.  ,\n",
       "       0.03, 0.02, 1.  , 0.02, 1.  , 1.  , 1.  , 1.  , 0.02, 0.01, 0.66,\n",
       "       0.99, 0.01, 0.  , 0.61, 0.  , 1.  , 0.08, 0.03, 0.11, 1.  , 0.02,\n",
       "       0.03, 1.  , 0.99, 1.  , 0.72, 0.01, 1.  , 1.  , 0.99, 0.96, 0.98,\n",
       "       0.99, 0.95, 0.03, 0.95, 0.07, 1.  , 0.98, 1.  , 0.  , 0.02, 1.  ,\n",
       "       0.04, 0.  , 0.06, 1.  , 1.  , 0.99, 0.06, 0.14, 0.01, 0.01, 0.99,\n",
       "       0.14, 1.  , 0.98, 1.  , 0.02, 1.  , 0.  , 0.08, 1.  , 0.09, 0.01,\n",
       "       0.99, 1.  , 1.  , 0.71, 0.99, 0.1 , 1.  , 0.03, 0.  , 0.09, 0.83,\n",
       "       1.  , 0.89, 1.  , 0.95, 0.  , 0.12, 0.01, 0.71, 1.  , 0.78, 1.  ,\n",
       "       0.02, 0.05, 0.02, 0.  , 0.  , 1.  , 1.  , 0.05, 1.  , 0.21, 1.  ,\n",
       "       0.13, 0.  , 0.  , 0.02, 0.02, 0.  , 0.01, 0.01, 0.04, 1.  , 1.  ,\n",
       "       1.  , 0.9 , 0.17, 0.99, 0.04, 0.83, 1.  , 0.96, 0.  , 0.64, 0.99,\n",
       "       1.  , 1.  , 1.  , 1.  , 0.98, 0.98, 0.01, 1.  , 1.  , 0.11, 1.  ,\n",
       "       0.  , 0.99, 0.01, 0.98, 0.05, 1.  , 1.  , 1.  , 0.  , 1.  , 0.98,\n",
       "       0.1 , 0.88, 1.  , 0.01, 0.66, 0.08, 0.66, 0.67, 0.01, 0.  , 0.99,\n",
       "       1.  , 1.  , 0.01, 0.22, 0.08, 0.2 , 1.  , 1.  , 0.  , 0.13, 1.  ,\n",
       "       0.95, 1.  , 0.86, 1.  , 1.  , 0.99, 1.  , 1.  , 1.  , 1.  , 0.82,\n",
       "       0.08, 0.  , 1.  , 0.  , 0.94, 0.16, 1.  , 0.  , 0.98, 1.  , 1.  ,\n",
       "       0.98, 0.95, 0.98, 0.01, 1.  , 0.16, 1.  , 1.  , 1.  , 0.01, 1.  ,\n",
       "       0.79, 0.  , 0.  , 1.  , 0.16, 1.  , 1.  , 0.14, 0.16, 1.  , 0.99,\n",
       "       1.  , 1.  , 1.  , 1.  , 1.  , 1.  , 0.03, 1.  , 1.  , 0.99, 0.01,\n",
       "       1.  , 0.02, 0.01, 1.  , 0.  , 0.08, 0.03, 0.97, 0.21, 0.1 , 0.86,\n",
       "       0.05, 0.02, 0.04, 0.75, 0.03, 1.  , 1.  , 0.  , 0.  , 0.  , 0.76,\n",
       "       0.01, 1.  , 1.  , 1.  , 0.99, 0.  , 1.  , 0.  , 0.  , 0.  , 0.99,\n",
       "       0.01, 0.  , 0.99, 0.02, 1.  , 0.01, 0.02, 0.98, 1.  , 1.  , 0.03,\n",
       "       1.  , 0.  , 1.  , 0.  , 0.08, 0.96, 1.  , 0.98, 0.06, 0.04, 0.02,\n",
       "       1.  , 0.9 , 0.01, 0.97, 0.2 , 1.  , 1.  , 0.12, 1.  , 0.  , 0.  ,\n",
       "       1.  , 1.  , 1.  , 0.03, 0.21, 0.01, 1.  , 1.  , 0.  , 0.07, 0.09,\n",
       "       0.14, 0.  , 0.89, 1.  , 0.01, 0.85, 0.01, 0.07, 0.  , 1.  , 0.03,\n",
       "       0.03, 0.  , 1.  , 0.81, 1.  , 0.98, 0.  , 0.97, 1.  , 1.  , 0.01,\n",
       "       1.  , 0.98, 1.  , 0.8 , 0.11, 0.98, 1.  , 0.02, 1.  , 1.  , 0.05,\n",
       "       0.01, 1.  , 0.98, 0.78, 0.  , 0.03, 0.  , 0.01, 0.04, 0.04, 0.01,\n",
       "       0.  , 1.  , 0.01, 0.14, 0.  , 1.  , 0.02, 0.69, 0.  , 1.  , 0.99,\n",
       "       0.03, 0.18, 0.15, 0.01, 0.  , 0.1 , 0.03, 0.07, 1.  , 0.03, 0.99,\n",
       "       0.01, 0.05, 0.  , 1.  , 0.01, 0.08]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.fit(train_data, use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_tuning_predicted = chain.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9725314368\n"
     ]
    }
   ],
   "source": [
    "bfr_tun_roc_auc = roc_auc(y_true=test_data.target,\n",
    "                          y_score=before_tuning_predicted.predict)\n",
    "print(round(bfr_tun_roc_auc, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.945054945054945, 0.8775510204081632, 0.9137055837563451)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = precision_score(test_data.target,before_tuning_predicted.predict.round())\n",
    "r = recall_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round())\n",
    "a = accuracy_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round())\n",
    "p, r, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_model_types.remove('knn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xgboost'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_model_types[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit : roc_auc is  0.9692846836\n",
      "logit : precision is  0.9565217391304348\n",
      "logit : recall is  0.8979591836734694\n",
      "logit : accuracy is  0.9289340101522843 \n",
      "\n",
      "lda : roc_auc is  0.9580498866\n",
      "lda : precision is  0.9876543209876543\n",
      "lda : recall is  0.8163265306122449\n",
      "lda : accuracy is  0.9035532994923858 \n",
      "\n",
      "qda : roc_auc is  0.9612451041\n",
      "qda : precision is  0.9354838709677419\n",
      "qda : recall is  0.8877551020408163\n",
      "qda : accuracy is  0.9137055837563451 \n",
      "\n",
      "dt : roc_auc is  0.9032673676\n",
      "dt : precision is  0.9540229885057471\n",
      "dt : recall is  0.8469387755102041\n",
      "dt : accuracy is  0.9035532994923858 \n",
      "\n",
      "rf : roc_auc is  0.9788188002\n",
      "rf : precision is  0.946236559139785\n",
      "rf : recall is  0.8979591836734694\n",
      "rf : accuracy is  0.9238578680203046 \n",
      "\n",
      "mlp : roc_auc is  0.9633065347\n",
      "mlp : precision is  0.9361702127659575\n",
      "mlp : recall is  0.8979591836734694\n",
      "mlp : accuracy is  0.9187817258883249 \n",
      "\n",
      "knn : roc_auc is  0.9545454545\n",
      "knn : precision is  0.9555555555555556\n",
      "knn : recall is  0.8775510204081632\n",
      "knn : accuracy is  0.9187817258883249 \n",
      "\n",
      "svc : roc_auc is  0.9642341785\n",
      "svc : precision is  0.9565217391304348\n",
      "svc : recall is  0.8979591836734694\n",
      "svc : accuracy is  0.9289340101522843 \n",
      "\n",
      "xgboost : roc_auc is  0.9735106164\n",
      "xgboost : precision is  0.9361702127659575\n",
      "xgboost : recall is  0.8979591836734694\n",
      "xgboost : accuracy is  0.9187817258883249 \n",
      "\n",
      "bernb : roc_auc is  0.9516594517\n",
      "bernb : precision is  0.9875\n",
      "bernb : recall is  0.8061224489795918\n",
      "bernb : accuracy is  0.8984771573604061 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in available_model_types[:-1]:\n",
    "    chain = get_simple_chain(model)\n",
    "    chain.fit(train_data, use_cache=False)\n",
    "    before_tuning_predicted = chain.predict(test_data)\n",
    "    bfr_tun_roc_auc = roc_auc(y_true=test_data.target,\n",
    "                          y_score=before_tuning_predicted.predict)\n",
    "    p = precision_score(test_data.target,before_tuning_predicted.predict.round())\n",
    "    r = recall_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round())\n",
    "    a = accuracy_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round())\n",
    "    print(model, ': roc_auc is ', round(bfr_tun_roc_auc, 10))\n",
    "    print(model, ': precision is ', p)\n",
    "    print(model, ': recall is ', r)\n",
    "    print(model, ': accuracy is ', a, '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = 'C:\\\\Users\\\\Тимур\\\\vir\\\\Scripts\\\\Fedot\\\\examples\\\\data\\\\creditcard\\\\train.csv'\n",
    "test_file_path = 'C:\\\\Users\\\\Тимур\\\\vir\\\\Scripts\\\\Fedot\\\\examples\\\\data\\\\creditcard\\\\test.csv'\n",
    "train_data = InputData.from_csv(train_file_path)\n",
    "test_data = InputData.from_csv(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model b not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-55b09e40f4f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mavailable_model_types\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mchain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_simple_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mchain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mbefore_tuning_predicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     bfr_tun_roc_auc = roc_auc(y_true=test_data.target,\n",
      "\u001b[1;32m~\\vir\\Scripts\\Fedot\\core\\composer\\chain.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, input_data, use_cache, verbose)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clean_model_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mtrain_predicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot_node\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrain_predicted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\vir\\Scripts\\Fedot\\core\\composer\\node.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, input_data, verbose)\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Trying to fit primary node with model: {self.model}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mmodel_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_using_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         return OutputData(idx=input_data.idx,\n",
      "\u001b[1;32m~\\vir\\Scripts\\Fedot\\core\\composer\\node.py\u001b[0m in \u001b[0;36m_fit_using_cache\u001b[1;34m(self, input_data, with_preprocessing, verbose)\u001b[0m\n\u001b[0;32m     63\u001b[0m         preprocessed_data = transformation_function_for_data(\n\u001b[0;32m     64\u001b[0m             \u001b[0minput_data_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             required_data_types=self.model.metadata.input_types)(input_data)\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactual_cached_state\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\vir\\Scripts\\Fedot\\core\\models\\model.py\u001b[0m in \u001b[0;36mmetadata\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mmodel_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelTypesRepository\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_info_by_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Model {self.model_type} not found'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Model b not found"
     ]
    }
   ],
   "source": [
    "for model in available_model_types[:-1]:\n",
    "    chain = get_simple_chain(model)\n",
    "    chain.fit(train_data, use_cache=False)\n",
    "    before_tuning_predicted = chain.predict(test_data)\n",
    "    bfr_tun_roc_auc = roc_auc(y_true=test_data.target,\n",
    "                          y_score=before_tuning_predicted.predict)\n",
    "    p = precision_score(test_data.target,before_tuning_predicted.predict.round())\n",
    "    r = recall_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round())\n",
    "    a = accuracy_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round())\n",
    "    print(model, ': roc_auc is ', round(bfr_tun_roc_auc, 10))\n",
    "    print(model, ': precision is ', p)\n",
    "    print(model, ': recall is ', r)\n",
    "    print(model, ': accuracy is ', a, '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = get_simple_chain('svc')\n",
    "chain.fit(train_data, use_cache=False)\n",
    "before_tuning_predicted = chain.predict(test_data)\n",
    "bfr_tun_roc_auc = roc_auc(y_true=test_data.target,\n",
    "                      y_score=before_tuning_predicted.predict)\n",
    "p = precision_score(test_data.target,before_tuning_predicted.predict.round())\n",
    "r = recall_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round())\n",
    "a = accuracy_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round())\n",
    "model = 'mod'\n",
    "print(model, ': roc_auc is ', round(bfr_tun_roc_auc, 10))\n",
    "print(model, ': precision is ', p)\n",
    "print(model, ': recall is ', r)\n",
    "print(model, ': accuracy is ', a, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9999999663, 0.9998596097150078, 1.0, 0.9999296554877513)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(round(bfr_tun_roc_auc, 10), p, r, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit : roc_auc is  0.9872483007\n",
      "logit : precision is  0.975940548072457\n",
      "logit : recall is  0.9219671440606572\n",
      "logit : accuracy is  0.9495190193974993 \n",
      "\n",
      "lda : roc_auc is  0.9752322276\n",
      "lda : precision is  0.9850909090909091\n",
      "lda : recall is  0.8558340353833193\n",
      "lda : accuracy is  0.9212844907936619 \n",
      "\n",
      "qda : roc_auc is  0.9748670447\n",
      "qda : precision is  0.9617988577480238\n",
      "qda : recall is  0.8926214546475709\n",
      "qda : accuracy is  0.9284420449149711 \n",
      "\n",
      "dt : roc_auc is  0.999753304\n",
      "dt : precision is  0.9995088063995509\n",
      "dt : recall is  1.0\n",
      "dt : accuracy is  0.9997537942071294 \n",
      "\n",
      "rf : roc_auc is  1.0\n",
      "rf : precision is  0.9999297999297999\n",
      "rf : recall is  1.0\n",
      "rf : accuracy is  0.9999648277438756 \n",
      "\n",
      "mlp : roc_auc is  0.9999723312\n",
      "mlp : precision is  0.9994036134011577\n",
      "mlp : recall is  1.0\n",
      "mlp : accuracy is  0.9997010358229429 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in available_model_types[:-1]:\n",
    "    chain = get_simple_chain(model)\n",
    "    chain.fit(train_data, use_cache=False)\n",
    "    before_tuning_predicted = chain.predict(test_data)\n",
    "    bfr_tun_roc_auc = roc_auc(y_true=test_data.target,\n",
    "                          y_score=before_tuning_predicted.predict)\n",
    "    p = precision_score(test_data.target,before_tuning_predicted.predict.round())\n",
    "    r = recall_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round())\n",
    "    a = accuracy_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round())\n",
    "    print(model, ': roc_auc is ', round(bfr_tun_roc_auc, 10))\n",
    "    print(model, ': precision is ', p)\n",
    "    print(model, ': recall is ', r)\n",
    "    print(model, ': accuracy is ', a, '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataPreprocess from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rob_scaler = RobustScaler()\n",
    "df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "\n",
    "df.drop(['Time','Amount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.783274</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.269825</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.983721</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.418291</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.670579</td>\n",
       "      <td>-0.994960</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
       "0       1.783274    -0.994983 -1.359807 -0.072781  2.536347  1.378155   \n",
       "1      -0.269825    -0.994983  1.191857  0.266151  0.166480  0.448154   \n",
       "2       4.983721    -0.994972 -1.358354 -1.340163  1.773209  0.379780   \n",
       "3       1.418291    -0.994972 -0.966272 -0.185226  1.792993 -0.863291   \n",
       "4       0.670579    -0.994960 -1.158233  0.877737  1.548718  0.403034   \n",
       "\n",
       "         V5        V6        V7        V8  ...       V20       V21       V22  \\\n",
       "0 -0.338321  0.462388  0.239599  0.098698  ...  0.251412 -0.018307  0.277838   \n",
       "1  0.060018 -0.082361 -0.078803  0.085102  ... -0.069083 -0.225775 -0.638672   \n",
       "2 -0.503198  1.800499  0.791461  0.247676  ...  0.524980  0.247998  0.771679   \n",
       "3 -0.010309  1.247203  0.237609  0.377436  ... -0.208038 -0.108300  0.005274   \n",
       "4 -0.407193  0.095921  0.592941 -0.270533  ...  0.408542 -0.009431  0.798278   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28  Class  \n",
       "0 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n",
       "1  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n",
       "2  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n",
       "3 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n",
       "4 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_amount = df['scaled_amount']\n",
    "scaled_time = df['scaled_time']\n",
    "\n",
    "df.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
    "df.insert(0, 'scaled_amount', scaled_amount)\n",
    "df.insert(1, 'scaled_time', scaled_time)\n",
    "\n",
    "# Amount and Time are Scaled!\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    284315\n",
       "1       492\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Class').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(sampling_strategy = 'all', random_state=42, replacement = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomOverSampler(sampling_strategy = 'all', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Class'])\n",
    "y = df.iloc[:,[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res, y_res = rus.fit_resample(X, y)\n",
    "X_res['Class'] = y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.783274</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.269825</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.983721</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.418291</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.670579</td>\n",
       "      <td>-0.994960</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568625</th>\n",
       "      <td>-0.286173</td>\n",
       "      <td>-0.589422</td>\n",
       "      <td>1.081234</td>\n",
       "      <td>0.416414</td>\n",
       "      <td>0.862919</td>\n",
       "      <td>2.520863</td>\n",
       "      <td>-0.005021</td>\n",
       "      <td>0.563341</td>\n",
       "      <td>-0.123372</td>\n",
       "      <td>0.223122</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165249</td>\n",
       "      <td>-0.159387</td>\n",
       "      <td>-0.305154</td>\n",
       "      <td>0.053620</td>\n",
       "      <td>0.011761</td>\n",
       "      <td>0.375146</td>\n",
       "      <td>-0.106299</td>\n",
       "      <td>0.021008</td>\n",
       "      <td>0.010559</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568626</th>\n",
       "      <td>-0.307413</td>\n",
       "      <td>-0.364595</td>\n",
       "      <td>-1.739341</td>\n",
       "      <td>1.344521</td>\n",
       "      <td>-0.534379</td>\n",
       "      <td>3.195291</td>\n",
       "      <td>-0.416196</td>\n",
       "      <td>-1.261961</td>\n",
       "      <td>-2.340991</td>\n",
       "      <td>0.713004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002749</td>\n",
       "      <td>0.383180</td>\n",
       "      <td>-0.213952</td>\n",
       "      <td>-0.336640</td>\n",
       "      <td>0.237076</td>\n",
       "      <td>0.246003</td>\n",
       "      <td>-0.044228</td>\n",
       "      <td>0.510729</td>\n",
       "      <td>0.220952</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568627</th>\n",
       "      <td>-0.041640</td>\n",
       "      <td>-0.587472</td>\n",
       "      <td>-0.860827</td>\n",
       "      <td>3.131790</td>\n",
       "      <td>-5.052968</td>\n",
       "      <td>5.420941</td>\n",
       "      <td>-2.494141</td>\n",
       "      <td>-1.811287</td>\n",
       "      <td>-5.479117</td>\n",
       "      <td>1.189472</td>\n",
       "      <td>...</td>\n",
       "      <td>1.085760</td>\n",
       "      <td>1.192694</td>\n",
       "      <td>0.090356</td>\n",
       "      <td>-0.341881</td>\n",
       "      <td>-0.215924</td>\n",
       "      <td>1.053032</td>\n",
       "      <td>0.271139</td>\n",
       "      <td>1.373300</td>\n",
       "      <td>0.691195</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568628</th>\n",
       "      <td>-0.293440</td>\n",
       "      <td>-0.521811</td>\n",
       "      <td>1.159373</td>\n",
       "      <td>2.844795</td>\n",
       "      <td>-4.050680</td>\n",
       "      <td>4.777701</td>\n",
       "      <td>2.948980</td>\n",
       "      <td>-2.010361</td>\n",
       "      <td>1.744086</td>\n",
       "      <td>-0.410287</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059264</td>\n",
       "      <td>-0.176541</td>\n",
       "      <td>-0.433470</td>\n",
       "      <td>-0.529323</td>\n",
       "      <td>-0.597020</td>\n",
       "      <td>1.335954</td>\n",
       "      <td>0.547092</td>\n",
       "      <td>0.009979</td>\n",
       "      <td>0.160769</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568629</th>\n",
       "      <td>-0.275554</td>\n",
       "      <td>0.209706</td>\n",
       "      <td>-1.456876</td>\n",
       "      <td>3.740306</td>\n",
       "      <td>-7.404518</td>\n",
       "      <td>7.440964</td>\n",
       "      <td>-1.549878</td>\n",
       "      <td>-1.661697</td>\n",
       "      <td>-5.757213</td>\n",
       "      <td>1.615011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.529557</td>\n",
       "      <td>0.957897</td>\n",
       "      <td>0.145339</td>\n",
       "      <td>-0.044704</td>\n",
       "      <td>-0.544962</td>\n",
       "      <td>-0.757757</td>\n",
       "      <td>-0.005352</td>\n",
       "      <td>0.318152</td>\n",
       "      <td>-0.323554</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568630 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
       "0            1.783274    -0.994983 -1.359807 -0.072781  2.536347  1.378155   \n",
       "1           -0.269825    -0.994983  1.191857  0.266151  0.166480  0.448154   \n",
       "2            4.983721    -0.994972 -1.358354 -1.340163  1.773209  0.379780   \n",
       "3            1.418291    -0.994972 -0.966272 -0.185226  1.792993 -0.863291   \n",
       "4            0.670579    -0.994960 -1.158233  0.877737  1.548718  0.403034   \n",
       "...               ...          ...       ...       ...       ...       ...   \n",
       "568625      -0.286173    -0.589422  1.081234  0.416414  0.862919  2.520863   \n",
       "568626      -0.307413    -0.364595 -1.739341  1.344521 -0.534379  3.195291   \n",
       "568627      -0.041640    -0.587472 -0.860827  3.131790 -5.052968  5.420941   \n",
       "568628      -0.293440    -0.521811  1.159373  2.844795 -4.050680  4.777701   \n",
       "568629      -0.275554     0.209706 -1.456876  3.740306 -7.404518  7.440964   \n",
       "\n",
       "              V5        V6        V7        V8  ...       V20       V21  \\\n",
       "0      -0.338321  0.462388  0.239599  0.098698  ...  0.251412 -0.018307   \n",
       "1       0.060018 -0.082361 -0.078803  0.085102  ... -0.069083 -0.225775   \n",
       "2      -0.503198  1.800499  0.791461  0.247676  ...  0.524980  0.247998   \n",
       "3      -0.010309  1.247203  0.237609  0.377436  ... -0.208038 -0.108300   \n",
       "4      -0.407193  0.095921  0.592941 -0.270533  ...  0.408542 -0.009431   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "568625 -0.005021  0.563341 -0.123372  0.223122  ... -0.165249 -0.159387   \n",
       "568626 -0.416196 -1.261961 -2.340991  0.713004  ...  0.002749  0.383180   \n",
       "568627 -2.494141 -1.811287 -5.479117  1.189472  ...  1.085760  1.192694   \n",
       "568628  2.948980 -2.010361  1.744086 -0.410287  ... -0.059264 -0.176541   \n",
       "568629 -1.549878 -1.661697 -5.757213  1.615011  ...  0.529557  0.957897   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0       0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1      -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2       0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3       0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4       0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "568625 -0.305154  0.053620  0.011761  0.375146 -0.106299  0.021008  0.010559   \n",
       "568626 -0.213952 -0.336640  0.237076  0.246003 -0.044228  0.510729  0.220952   \n",
       "568627  0.090356 -0.341881 -0.215924  1.053032  0.271139  1.373300  0.691195   \n",
       "568628 -0.433470 -0.529323 -0.597020  1.335954  0.547092  0.009979  0.160769   \n",
       "568629  0.145339 -0.044704 -0.544962 -0.757757 -0.005352  0.318152 -0.323554   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "568625      1  \n",
       "568626      1  \n",
       "568627      1  \n",
       "568628      1  \n",
       "568629      1  \n",
       "\n",
       "[568630 rows x 31 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df_1 = shuffle(X_res, random_state = 42).reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.813526</td>\n",
       "      <td>0.303810</td>\n",
       "      <td>-2.450367</td>\n",
       "      <td>2.107729</td>\n",
       "      <td>-5.140663</td>\n",
       "      <td>1.411304</td>\n",
       "      <td>-1.690780</td>\n",
       "      <td>-0.736427</td>\n",
       "      <td>-3.657946</td>\n",
       "      <td>1.944906</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130438</td>\n",
       "      <td>0.800538</td>\n",
       "      <td>0.364617</td>\n",
       "      <td>0.233608</td>\n",
       "      <td>-0.282078</td>\n",
       "      <td>-0.320311</td>\n",
       "      <td>0.492920</td>\n",
       "      <td>0.359976</td>\n",
       "      <td>-0.115471</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.293440</td>\n",
       "      <td>-0.501662</td>\n",
       "      <td>-4.566342</td>\n",
       "      <td>3.353451</td>\n",
       "      <td>-4.572028</td>\n",
       "      <td>3.616119</td>\n",
       "      <td>-2.493138</td>\n",
       "      <td>-1.090000</td>\n",
       "      <td>-5.551433</td>\n",
       "      <td>0.447783</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.249128</td>\n",
       "      <td>2.674466</td>\n",
       "      <td>-0.020880</td>\n",
       "      <td>-0.302447</td>\n",
       "      <td>-0.086396</td>\n",
       "      <td>-0.516060</td>\n",
       "      <td>-0.295102</td>\n",
       "      <td>0.195985</td>\n",
       "      <td>0.141115</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.119192</td>\n",
       "      <td>-0.944854</td>\n",
       "      <td>-0.851994</td>\n",
       "      <td>0.935553</td>\n",
       "      <td>2.188136</td>\n",
       "      <td>0.709286</td>\n",
       "      <td>0.178930</td>\n",
       "      <td>-0.349335</td>\n",
       "      <td>0.857609</td>\n",
       "      <td>-0.416252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270933</td>\n",
       "      <td>-0.411022</td>\n",
       "      <td>-0.404762</td>\n",
       "      <td>-0.096893</td>\n",
       "      <td>0.404476</td>\n",
       "      <td>0.377388</td>\n",
       "      <td>0.344173</td>\n",
       "      <td>-0.000311</td>\n",
       "      <td>-0.125090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.798225</td>\n",
       "      <td>0.544320</td>\n",
       "      <td>0.469750</td>\n",
       "      <td>-1.237555</td>\n",
       "      <td>-1.767341</td>\n",
       "      <td>4.833490</td>\n",
       "      <td>-0.268715</td>\n",
       "      <td>-0.512760</td>\n",
       "      <td>1.140149</td>\n",
       "      <td>-0.341273</td>\n",
       "      <td>...</td>\n",
       "      <td>1.277315</td>\n",
       "      <td>0.303905</td>\n",
       "      <td>-0.647075</td>\n",
       "      <td>-0.373014</td>\n",
       "      <td>0.260801</td>\n",
       "      <td>-0.496566</td>\n",
       "      <td>-0.245973</td>\n",
       "      <td>-0.117858</td>\n",
       "      <td>0.144774</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.191434</td>\n",
       "      <td>0.639399</td>\n",
       "      <td>-3.975939</td>\n",
       "      <td>-1.244939</td>\n",
       "      <td>-3.707414</td>\n",
       "      <td>4.544772</td>\n",
       "      <td>4.050676</td>\n",
       "      <td>-3.407679</td>\n",
       "      <td>-5.063118</td>\n",
       "      <td>1.007042</td>\n",
       "      <td>...</td>\n",
       "      <td>2.109403</td>\n",
       "      <td>1.059737</td>\n",
       "      <td>-0.037395</td>\n",
       "      <td>0.348707</td>\n",
       "      <td>-0.162929</td>\n",
       "      <td>0.410531</td>\n",
       "      <td>-0.123612</td>\n",
       "      <td>0.877424</td>\n",
       "      <td>0.667568</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568625</th>\n",
       "      <td>1.578984</td>\n",
       "      <td>-0.152046</td>\n",
       "      <td>0.960530</td>\n",
       "      <td>-1.006608</td>\n",
       "      <td>0.799673</td>\n",
       "      <td>-1.216255</td>\n",
       "      <td>-1.541568</td>\n",
       "      <td>-0.670999</td>\n",
       "      <td>-0.610605</td>\n",
       "      <td>0.058303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171371</td>\n",
       "      <td>0.254714</td>\n",
       "      <td>0.653521</td>\n",
       "      <td>-0.204047</td>\n",
       "      <td>0.621861</td>\n",
       "      <td>0.370590</td>\n",
       "      <td>0.070796</td>\n",
       "      <td>0.015218</td>\n",
       "      <td>0.037579</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568626</th>\n",
       "      <td>0.939426</td>\n",
       "      <td>0.872990</td>\n",
       "      <td>-0.775981</td>\n",
       "      <td>0.144023</td>\n",
       "      <td>-1.142399</td>\n",
       "      <td>-1.241113</td>\n",
       "      <td>1.940358</td>\n",
       "      <td>3.912076</td>\n",
       "      <td>-0.466107</td>\n",
       "      <td>1.360620</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.295730</td>\n",
       "      <td>0.037078</td>\n",
       "      <td>-0.019575</td>\n",
       "      <td>0.241830</td>\n",
       "      <td>0.682820</td>\n",
       "      <td>-1.635109</td>\n",
       "      <td>-0.770941</td>\n",
       "      <td>0.066006</td>\n",
       "      <td>0.137056</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568627</th>\n",
       "      <td>-0.292042</td>\n",
       "      <td>-0.262585</td>\n",
       "      <td>-5.267760</td>\n",
       "      <td>2.506719</td>\n",
       "      <td>-5.290925</td>\n",
       "      <td>4.886134</td>\n",
       "      <td>-3.343188</td>\n",
       "      <td>-1.100085</td>\n",
       "      <td>-5.810509</td>\n",
       "      <td>1.726343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286043</td>\n",
       "      <td>0.764266</td>\n",
       "      <td>0.473262</td>\n",
       "      <td>0.548482</td>\n",
       "      <td>-0.156850</td>\n",
       "      <td>-0.710187</td>\n",
       "      <td>-0.366423</td>\n",
       "      <td>-1.486766</td>\n",
       "      <td>0.677664</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568628</th>\n",
       "      <td>-0.252358</td>\n",
       "      <td>-0.057531</td>\n",
       "      <td>-0.146609</td>\n",
       "      <td>0.992946</td>\n",
       "      <td>1.524591</td>\n",
       "      <td>0.485774</td>\n",
       "      <td>0.349308</td>\n",
       "      <td>-0.815198</td>\n",
       "      <td>1.076640</td>\n",
       "      <td>-0.395316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007155</td>\n",
       "      <td>0.052649</td>\n",
       "      <td>0.354089</td>\n",
       "      <td>-0.291198</td>\n",
       "      <td>0.402849</td>\n",
       "      <td>0.237383</td>\n",
       "      <td>-0.398467</td>\n",
       "      <td>-0.121139</td>\n",
       "      <td>-0.196195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568629</th>\n",
       "      <td>1.271571</td>\n",
       "      <td>-0.097640</td>\n",
       "      <td>1.233174</td>\n",
       "      <td>-0.784851</td>\n",
       "      <td>0.386784</td>\n",
       "      <td>-0.698559</td>\n",
       "      <td>-1.034018</td>\n",
       "      <td>-0.637028</td>\n",
       "      <td>-0.502369</td>\n",
       "      <td>-0.188057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337732</td>\n",
       "      <td>0.027634</td>\n",
       "      <td>-0.234522</td>\n",
       "      <td>-0.059544</td>\n",
       "      <td>-0.109073</td>\n",
       "      <td>0.290326</td>\n",
       "      <td>-0.393074</td>\n",
       "      <td>0.001217</td>\n",
       "      <td>0.038588</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568630 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
       "0            0.813526     0.303810 -2.450367  2.107729 -5.140663  1.411304   \n",
       "1           -0.293440    -0.501662 -4.566342  3.353451 -4.572028  3.616119   \n",
       "2            0.119192    -0.944854 -0.851994  0.935553  2.188136  0.709286   \n",
       "3            9.798225     0.544320  0.469750 -1.237555 -1.767341  4.833490   \n",
       "4           -0.191434     0.639399 -3.975939 -1.244939 -3.707414  4.544772   \n",
       "...               ...          ...       ...       ...       ...       ...   \n",
       "568625       1.578984    -0.152046  0.960530 -1.006608  0.799673 -1.216255   \n",
       "568626       0.939426     0.872990 -0.775981  0.144023 -1.142399 -1.241113   \n",
       "568627      -0.292042    -0.262585 -5.267760  2.506719 -5.290925  4.886134   \n",
       "568628      -0.252358    -0.057531 -0.146609  0.992946  1.524591  0.485774   \n",
       "568629       1.271571    -0.097640  1.233174 -0.784851  0.386784 -0.698559   \n",
       "\n",
       "              V5        V6        V7        V8  ...       V20       V21  \\\n",
       "0      -1.690780 -0.736427 -3.657946  1.944906  ... -0.130438  0.800538   \n",
       "1      -2.493138 -1.090000 -5.551433  0.447783  ... -0.249128  2.674466   \n",
       "2       0.178930 -0.349335  0.857609 -0.416252  ...  0.270933 -0.411022   \n",
       "3      -0.268715 -0.512760  1.140149 -0.341273  ...  1.277315  0.303905   \n",
       "4       4.050676 -3.407679 -5.063118  1.007042  ...  2.109403  1.059737   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "568625 -1.541568 -0.670999 -0.610605  0.058303  ...  0.171371  0.254714   \n",
       "568626  1.940358  3.912076 -0.466107  1.360620  ... -0.295730  0.037078   \n",
       "568627 -3.343188 -1.100085 -5.810509  1.726343  ... -0.286043  0.764266   \n",
       "568628  0.349308 -0.815198  1.076640 -0.395316  ...  0.007155  0.052649   \n",
       "568629 -1.034018 -0.637028 -0.502369 -0.188057  ...  0.337732  0.027634   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0       0.364617  0.233608 -0.282078 -0.320311  0.492920  0.359976 -0.115471   \n",
       "1      -0.020880 -0.302447 -0.086396 -0.516060 -0.295102  0.195985  0.141115   \n",
       "2      -0.404762 -0.096893  0.404476  0.377388  0.344173 -0.000311 -0.125090   \n",
       "3      -0.647075 -0.373014  0.260801 -0.496566 -0.245973 -0.117858  0.144774   \n",
       "4      -0.037395  0.348707 -0.162929  0.410531 -0.123612  0.877424  0.667568   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "568625  0.653521 -0.204047  0.621861  0.370590  0.070796  0.015218  0.037579   \n",
       "568626 -0.019575  0.241830  0.682820 -1.635109 -0.770941  0.066006  0.137056   \n",
       "568627  0.473262  0.548482 -0.156850 -0.710187 -0.366423 -1.486766  0.677664   \n",
       "568628  0.354089 -0.291198  0.402849  0.237383 -0.398467 -0.121139 -0.196195   \n",
       "568629 -0.234522 -0.059544 -0.109073  0.290326 -0.393074  0.001217  0.038588   \n",
       "\n",
       "        Class  \n",
       "0           1  \n",
       "1           1  \n",
       "2           0  \n",
       "3           1  \n",
       "4           1  \n",
       "...       ...  \n",
       "568625      0  \n",
       "568626      0  \n",
       "568627      1  \n",
       "568628      0  \n",
       "568629      0  \n",
       "\n",
       "[568630 rows x 31 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.to_csv(r'./creditcard_scaling_overSample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
