{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from core.composer.gp_composer.gp_composer import \\\n",
    "    GPComposer, GPComposerRequirements\n",
    "from core.composer.visualisation import ComposerVisualiser\n",
    "from core.repository.model_types_repository import ModelTypesRepository\n",
    "from core.repository.quality_metrics_repository import \\\n",
    "    ClassificationMetricsEnum, MetricsRepository\n",
    "from core.repository.tasks import Task, TaskTypesEnum\n",
    "from core.utils import probs_to_labels\n",
    "from examples.utils import create_multi_clf_examples_from_excel\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score as roc_auc\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report, confusion_matrix\n",
    "from benchmark.benchmark_utils import get_scoring_case_data_paths\n",
    "from core.composer.chain import Chain\n",
    "from core.composer.node import PrimaryNode, SecondaryNode\n",
    "from core.models.data import InputData\n",
    "\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "def get_model(train_file_path: str, cur_lead_time: datetime.timedelta = timedelta(minutes=5)):\n",
    "    task = Task(task_type=TaskTypesEnum.classification)\n",
    "    dataset_to_compose = InputData.from_csv(train_file_path, task=task)\n",
    "\n",
    "    # the search of the models provided by the framework\n",
    "    # that can be used as nodes in a chain for the selected task\n",
    "    models_repo = ModelTypesRepository()\n",
    "    available_model_types, _ = models_repo.suitable_model(task_type=task.task_type)\n",
    "\n",
    "    metric_function = MetricsRepository(). \\\n",
    "        metric_by_id(ClassificationMetricsEnum.ROCAUC_penalty)\n",
    "\n",
    "    composer_requirements = GPComposerRequirements(\n",
    "        primary=available_model_types, secondary=available_model_types,\n",
    "        max_lead_time=cur_lead_time, max_arity=3,\n",
    "        max_depth=4, pop_size=20, num_of_generations=100, \n",
    "        crossover_prob = 0.8, mutation_prob = 0.8, \n",
    "        add_single_model_chains = True)\n",
    "\n",
    "    # Create the genetic programming-based composer, that allow to find\n",
    "    # the optimal structure of the composite model\n",
    "    composer = GPComposer()\n",
    "\n",
    "    # run the search of best suitable model\n",
    "    chain_evo_composed = composer.compose_chain(data=dataset_to_compose,\n",
    "                                                initial_chain=None,\n",
    "                                                composer_requirements=composer_requirements,\n",
    "                                                metrics=metric_function, is_visualise=False)\n",
    "    chain_evo_composed.fit(input_data=dataset_to_compose)\n",
    "\n",
    "    return chain_evo_composed\n",
    "\n",
    "\n",
    "def apply_model_to_data(model: Chain, data_path: str):\n",
    "    df, file_path = create_multi_clf_examples_from_excel(data_path, return_df=True)\n",
    "    dataset_to_apply = InputData.from_csv(file_path, with_target=True)\n",
    "    evo_predicted = model.predict(dataset_to_apply)\n",
    "    df['forecast'] = probs_to_labels(evo_predicted.predict)\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_model_to_data_and_predict(model: Chain, data_path: str):\n",
    "    df, file_path = create_multi_clf_examples_from_excel(data_path, return_df=True)\n",
    "    \n",
    "    dataset_to_validate = InputData.from_csv(data_path)\n",
    "    predicted_labels = model.predict(dataset_to_validate).predict\n",
    "    \n",
    "    \n",
    "    test_data = InputData.from_csv(file_path, with_target=True)\n",
    "    roc_auc_valid = roc_auc(y_true=test_data.target,\n",
    "                                  y_score=predicted_labels,\n",
    "                                  multi_class='ovo',\n",
    "                                  average='macro')\n",
    "    \n",
    "    roc_auc_st = roc_auc(y_true=test_data.target, y_score=predicted_labels.round())\n",
    "    \n",
    "    p = precision_score(y_true=test_data.target,y_pred=predicted_labels.round())\n",
    "    r = recall_score(y_true=test_data.target, y_pred=predicted_labels.round())\n",
    "    a = accuracy_score(y_true=test_data.target, y_pred=predicted_labels.round())\n",
    "    \n",
    "    return roc_auc_valid, roc_auc_st, p, r, a\n",
    "\n",
    "\n",
    "def validate_model_quality(model: Chain, data_path: str):\n",
    "    dataset_to_validate = InputData.from_csv(data_path)\n",
    "    predicted_labels = model.predict(dataset_to_validate).predict\n",
    "\n",
    "    roc_auc_valid = round(roc_auc(y_true=test_data.target,\n",
    "                                  y_score=predicted_labels,\n",
    "                                  multi_class='ovo',\n",
    "                                  average='macro'), 3)\n",
    "    \n",
    "    roc_auc_st = roc_auc(y_true=test_data.target,y_score=predicted_labels)\n",
    "                              \n",
    "    p = precision_score(y_true=test_data.target,y_pred=predicted_labels.round())\n",
    "    r = recall_score(y_true=test_data.target, y_pred=predicted_labels.round())\n",
    "    a = accuracy_score(y_true=test_data.target, y_pred=predicted_labels.round())\n",
    "    \n",
    "    return roc_auc_valid, roc_auc_st, p, r, a\n",
    "\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_chain():\n",
    "    first = PrimaryNode(model_type='knn')\n",
    "    chain = Chain(first)\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_first = r'./creditcard_scaling_underSample.csv'\n",
    "create_multi_clf_examples_from_excel(file_path_first, return_df = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = r'./examples/data/creditcard_scaling_underSample/creditcard_scaling_underSample.csv'\n",
    "test_file_path = r'./examples/data/creditcard_scaling_underSample/creditcard_scaling_underSample.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = InputData.from_csv(train_file_path)\n",
    "test_data = InputData.from_csv(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = get_simple_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.58692240715027\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "chain.fit(train_data, use_cache=False)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_tuning_predicted = chain.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfr_tun_roc_auc = round(roc_auc(y_true=test_data.target,\n",
    "                          y_score=before_tuning_predicted.predict), 4)\n",
    "\n",
    "p = round(precision_score(test_data.target,before_tuning_predicted.predict.round()), 4)\n",
    "r = round(recall_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round()), 4)\n",
    "a = round(accuracy_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round()),4 )\n",
    "f = round(f1_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round()), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.9896 \n",
      "PRECISION = 0.9477 \n",
      "RECALL = 0.9207 \n",
      "ACCURACY = 0.935 \n",
      "f1_score = 0.934\n"
     ]
    }
   ],
   "source": [
    "print(f'ROC_AUC = {bfr_tun_roc_auc} \\nPRECISION = {p} \\nRECALL = {r} \\nACCURACY = {a} \\nf1_score = {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Профайлинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "\n",
    "\n",
    "def profile(func):\n",
    "    \"\"\"Decorator for run function profile\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        profile_filename = func.__name__ + '.prof'\n",
    "        profiler = cProfile.Profile()\n",
    "        result = profiler.runcall(func, *args, **kwargs)\n",
    "        profiler.dump_stats(profile_filename)\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@profile\n",
    "def knn_fit():\n",
    "    \n",
    "    def get_simple_chain():\n",
    "        first = PrimaryNode(model_type='knn')\n",
    "        chain = Chain(first)\n",
    "\n",
    "        return chain\n",
    "\n",
    "    chain = get_simple_chain()\n",
    "    \n",
    "    train_data = InputData.from_csv(r'./examples/data/creditcard_scaling_underSample/creditcard_scaling_underSample.csv')\n",
    "    \n",
    "    chain.fit(train_data, use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "\n",
    "\n",
    "def profile(func):\n",
    "    \"\"\"Decorator for run function profile\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        profile_filename = func.__name__ + '.prof'\n",
    "        profiler = cProfile.Profile()\n",
    "        result = profiler.runcall(func, *args, **kwargs)\n",
    "        profiler.dump_stats(profile_filename)\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@profile\n",
    "def knn_fit_1():\n",
    "    \n",
    "    chain.fit(train_data, use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_fit_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying FEDOT model to full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = InputData.from_csv(r'./creditcard_scaling.csv')\n",
    "before_tuning_predicted = chain.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfr_tun_roc_auc = round(roc_auc(y_true=test_data.target,\n",
    "                          y_score=before_tuning_predicted.predict), 4)\n",
    "\n",
    "p = round(precision_score(test_data.target,before_tuning_predicted.predict.round()), 4)\n",
    "r = round(recall_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round()), 4)\n",
    "a = round(accuracy_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round()), 4)\n",
    "f = round(f1_score(y_true=test_data.target, y_pred=before_tuning_predicted.predict.round()), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.9741 \n",
      "PRECISION = 0.0329 \n",
      "RECALL = 0.8963 \n",
      "ACCURACY = 0.9543 \n",
      "f1_score = 0.0635\n"
     ]
    }
   ],
   "source": [
    "print(f'ROC_AUC = {bfr_tun_roc_auc} \\nPRECISION = {p} \\nRECALL = {r} \\nACCURACY = {a} \\nf1_score = {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = r'./examples/data/creditcard_scaling_underSample/train.csv'\n",
    "test_file_path = r'./examples/data/creditcard_scaling_underSample/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(train_file_path)\n",
    "df_test = pd.read_csv(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns = ['Class'])\n",
    "y_train = df_train.iloc[:,-1]\n",
    "\n",
    "X_test = df_test.drop(columns = ['Class'])\n",
    "y_test = df_test.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004996538162231445\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "neigh.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = neigh.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfr_tun_roc_auc = round(roc_auc(y_true=y_test, y_score=y_pred), 4)\n",
    "\n",
    "p = round(precision_score(y_test, y_pred), 4)\n",
    "r = round(recall_score(y_test, y_pred), 4)\n",
    "a = round(accuracy_score(y_test, y_pred), 4)\n",
    "f = round(f1_score(y_test, y_pred), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.9268 \n",
      "PRECISION = 0.9294 \n",
      "RECALL = 0.908 \n",
      "ACCURACY = 0.9289 \n",
      "f1_score = 0.9186\n"
     ]
    }
   ],
   "source": [
    "print(f'ROC_AUC = {bfr_tun_roc_auc} \\nPRECISION = {p} \\nRECALL = {r} \\nACCURACY = {a} \\nf1_score = {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying SKLEARN model to full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'./creditcard_scaling.csv')\n",
    "X_all = data.drop(columns=['Class'])\n",
    "y_all = data.iloc[:,-1]\n",
    "y_pred_all = neigh.predict(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfr_tun_roc_auc = round(roc_auc(y_all, y_pred_all), 4)\n",
    "\n",
    "p = round(precision_score(y_all, y_pred_all), 4)\n",
    "r = round(recall_score(y_all, y_pred_all), 4)\n",
    "a = round(accuracy_score(y_all, y_pred_all), 4)\n",
    "f = round(f1_score(y_all, y_pred_all), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.9317 \n",
      "PRECISION = 0.027 \n",
      "RECALL = 0.9207 \n",
      "ACCURACY = 0.9426 \n",
      "f1_score = 0.0525\n"
     ]
    }
   ],
   "source": [
    "print(f'ROC_AUC = {bfr_tun_roc_auc} \\nPRECISION = {p} \\nRECALL = {r} \\nACCURACY = {a} \\nf1_score = {f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
